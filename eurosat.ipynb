{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqeN1ascLiX6",
    "outputId": "0d249f29-95f3-4e89-d2a0-b24afaf41660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snntorch\n",
      "  Downloading snntorch-0.8.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/125.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.1+cu121)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.25.2)\n",
      "Collecting nir (from snntorch)\n",
      "  Downloading nir-1.0.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nirtorch (from snntorch)\n",
      "  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m974.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->snntorch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nir, nvidia-cusolver-cu12, nirtorch, snntorch\n",
      "Successfully installed nir-1.0.1 nirtorch-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 snntorch-0.8.1\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.2.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.10.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.13.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch_lightning) (12.4.127)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.11.2 pytorch_lightning-2.2.1 torchmetrics-1.3.2\n",
      "Collecting lightning-bolts\n",
      "  Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.25.2)\n",
      "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
      "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.3.2)\n",
      "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.2)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.17.1+cu121)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.15.2)\n",
      "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (67.7.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.10.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.2.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2023.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.62.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (2.31.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.13.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.4.127)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.9.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (2.1.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->lightning-bolts) (3.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
      "Installing collected packages: pytorch-lightning, lightning-bolts\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.1\n",
      "    Uninstalling pytorch-lightning-2.2.1:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.1\n",
      "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install snntorch\n",
    "!pip install pytorch_lightning\n",
    "!pip install lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\roberta\\anaconda3\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/294.9 kB 660.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/294.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.9/294.9 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuixrerL_2n_",
    "outputId": "01f525d3-1e03-4a97-daba-9f73c076716b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import resnet\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pytorch_lightning import LightningModule\n",
    "from pl_bolts.metrics import precision_at_k\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WTpr3_RBBiZ",
    "outputId": "aacd8fec-3367-40ed-decd-4f409a0b299d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3KzHGrgDF-R",
    "outputId": "dced9214-2555-48aa-e1ec-5c1bbf774c93"
   },
   "outputs": [],
   "source": [
    "#!unzip gdrive/MyDrive/EuroSAT_RGB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OoreJqp2_iF_"
   },
   "outputs": [],
   "source": [
    "class EurosatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = []\n",
    "        for root, dirs, files in os.walk(root):\n",
    "          for file in files:\n",
    "            self.samples.append(root+'/'+file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples[index]\n",
    "        img = Image.open(path)\n",
    "\n",
    "        target = self.class_to_idx[Path(path).parts[-2]]\n",
    "\n",
    "        if self.transform is not None:\n",
    "             img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMw0kW-WO21v",
    "outputId": "807b4d9b-05f4-4e0e-d5ed-1876a0bfa8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "W6CxlUVBCHJT"
   },
   "outputs": [],
   "source": [
    "trans_comp = T.Compose([\n",
    "  T.ToTensor(),\n",
    "  T.Normalize((0,), (1,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UcLS-BgyN2q_"
   },
   "outputs": [],
   "source": [
    "dataset = EurosatDataset(root = 'C:\\\\Users\\\\rober\\\\EuroSAT_RGB', transform=trans_comp)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.9, 0.1])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle= True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "j2D2JCTqONV5",
    "outputId": "5c5b156a-beb0-47d4-87be-2bd2efdf2e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x157357d1e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_features.size())\n",
    "\n",
    "print(train_labels.size())\n",
    "img = train_features[1].squeeze()\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ti59QdIIvREQ"
   },
   "outputs": [],
   "source": [
    "#class for the pretrained model\n",
    "class MocoV2(LightningModule):\n",
    "\n",
    "    def __init__(self, base_encoder, emb_dim, num_negatives, emb_spaces=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # create the encoders\n",
    "        template_model = getattr(torchvision.models, base_encoder)\n",
    "        self.encoder_q = template_model(num_classes=self.hparams.emb_dim)\n",
    "        self.encoder_k = template_model(num_classes=self.hparams.emb_dim)\n",
    "\n",
    "        # remove fc layer\n",
    "        self.encoder_q = nn.Sequential(*list(self.encoder_q.children())[:-1], nn.Flatten())\n",
    "        self.encoder_k = nn.Sequential(*list(self.encoder_k.children())[:-1], nn.Flatten())\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the projection heads\n",
    "        self.mlp_dim = 512 * (1 if base_encoder in ['resnet18', 'resnet34'] else 4)\n",
    "        self.heads_q = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(self.mlp_dim, self.mlp_dim), nn.ReLU(), nn.Linear(self.mlp_dim, emb_dim))\n",
    "            for _ in range(emb_spaces)\n",
    "        ])\n",
    "        self.heads_k = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(self.mlp_dim, self.mlp_dim), nn.ReLU(), nn.Linear(self.mlp_dim, emb_dim))\n",
    "            for _ in range(emb_spaces)\n",
    "        ])\n",
    "\n",
    "        for param_q, param_k in zip(self.heads_q.parameters(), self.heads_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # initialize\n",
    "            param_k.requires_grad = False  # not update by gradient\n",
    "\n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(emb_spaces, emb_dim, num_negatives))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=1)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(emb_spaces, 1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            em = self.hparams.encoder_momentum\n",
    "            param_k.data = param_k.data * em + param_q.data * (1. - em)\n",
    "        for param_q, param_k in zip(self.heads_q.parameters(), self.heads_k.parameters()):\n",
    "            em = self.hparams.encoder_momentum\n",
    "            param_k.data = param_k.data * em + param_q.data * (1. - em)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys, queue_idx):\n",
    "        # gather keys before updating queue\n",
    "        if self.use_ddp or self.use_ddp2:\n",
    "            keys = concat_all_gather(keys)\n",
    "\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr[queue_idx])\n",
    "        assert self.hparams.num_negatives % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[queue_idx, :, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.hparams.num_negatives  # move pointer\n",
    "\n",
    "        self.queue_ptr[queue_idx] = ptr\n",
    "\n",
    "    def forward(self, img_q, img_k):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            img_q: a batch of query images\n",
    "            img_k: a batch of key images\n",
    "        Output:\n",
    "            logits, targets\n",
    "        \"\"\"\n",
    "\n",
    "        # update the key encoder\n",
    "        self._momentum_update_key_encoder()\n",
    "\n",
    "        # compute query features\n",
    "        v_q = self.encoder_q(img_q)\n",
    "\n",
    "        # compute key features\n",
    "        v_k = []\n",
    "        for i in range(self.hparams.emb_spaces):\n",
    "            # shuffle for making use of BN\n",
    "            if self.use_ddp or self.use_ddp2:\n",
    "                img_k[i], idx_unshuffle = batch_shuffle_ddp(img_k[i])\n",
    "\n",
    "            with torch.no_grad():  # no gradient to keys\n",
    "                v_k.append(self.encoder_k(img_k[i]))\n",
    "\n",
    "            # undo shuffle\n",
    "            if self.use_ddp or self.use_ddp2:\n",
    "                v_k[i] = batch_unshuffle_ddp(v_k[i], idx_unshuffle)\n",
    "\n",
    "        logits = []\n",
    "        for i in range(self.hparams.emb_spaces):\n",
    "            # compute query projections\n",
    "            z_q = self.heads_q[i](v_q)  # queries: NxC\n",
    "            z_q = nn.functional.normalize(z_q, dim=1)\n",
    "\n",
    "            # compute key projections\n",
    "            z_k = []\n",
    "            for j in range(self.hparams.emb_spaces):\n",
    "                with torch.no_grad():  # no gradient to keys\n",
    "                    z_k.append(self.heads_k[i](v_k[j]))  # keys: NxC\n",
    "                    z_k[j] = nn.functional.normalize(z_k[j], dim=1)\n",
    "\n",
    "            # select positive and negative pairs\n",
    "            z_pos = z_k[i]\n",
    "            z_neg = self.queue[i].clone().detach()\n",
    "            if i > 0:  # embedding space 0 is invariant to all augmentations\n",
    "                z_neg = torch.cat([z_neg, *[z_k[j].T for j in range(self.hparams.emb_spaces) if j != i]], dim=1)\n",
    "\n",
    "            # compute logits\n",
    "            # Einstein sum is more intuitive\n",
    "            l_pos = torch.einsum('nc,nc->n', z_q, z_pos).unsqueeze(-1)  # positive logits: Nx1\n",
    "            l_neg = torch.einsum('nc,ck->nk', z_q, z_neg)  # negative logits: NxK\n",
    "\n",
    "            l = torch.cat([l_pos, l_neg], dim=1)  # logits: Nx(1+K)\n",
    "            l /= self.hparams.softmax_temperature  # apply temperature\n",
    "            logits.append(l)\n",
    "\n",
    "            # dequeue and enqueue\n",
    "            self._dequeue_and_enqueue(z_k[i], queue_idx=i)\n",
    "\n",
    "        # targets: positive key indicators\n",
    "        targets = torch.zeros(logits[0].shape[0], dtype=torch.long)\n",
    "        targets = targets.type_as(logits[0])\n",
    "\n",
    "        return logits, targets\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img_q, img_k = batch\n",
    "        if self.hparams.emb_spaces == 1 and isinstance(img_k, torch.Tensor):\n",
    "            img_k = [img_k]\n",
    "\n",
    "        output, target = self(img_q, img_k)\n",
    "\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for out in output:\n",
    "            losses.append(F.cross_entropy(out.float(), target.long()))\n",
    "            accuracies.append(precision_at_k(out, target, top_k=(1,))[0])\n",
    "        loss = torch.sum(torch.stack(losses))\n",
    "\n",
    "        log = {'train_loss': loss}\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            log[f'train_acc/subspace{i}'] = acc\n",
    "\n",
    "        self.log_dict(log, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = chain(self.encoder_q.parameters(), self.heads_q.parameters())\n",
    "        optimizer = optim.SGD(params, self.hparams.learning_rate,\n",
    "                              momentum=self.hparams.momentum,\n",
    "                              weight_decay=self.hparams.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--base_encoder', type=str, default='resnet18')\n",
    "        parser.add_argument('--emb_dim', type=int, default=128)\n",
    "        parser.add_argument('--num_workers', type=int, default=32)\n",
    "        parser.add_argument('--num_negatives', type=int, default=16384)\n",
    "        parser.add_argument('--encoder_momentum', type=float, default=0.999)\n",
    "        parser.add_argument('--softmax_temperature', type=float, default=0.07)\n",
    "        parser.add_argument('--learning_rate', type=float, default=0.03)\n",
    "        parser.add_argument('--momentum', type=float, default=0.9)\n",
    "        parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
    "        parser.add_argument('--batch_size', type=int, default=256)\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlauGo2fm6iy",
    "outputId": "9032c19b-1c1d-437b-ce5e-4f150774e8e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\envs\\gpu4\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:195: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.4 to v1.9.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\rober\\seco_resnet18_1m.ckpt`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (9): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MocoV2.load_from_checkpoint(\"C:\\\\Users\\\\rober\\\\seco_resnet18_1m.ckpt\")\n",
    "backbone = deepcopy(model.encoder_q)\n",
    "backbone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-kE0BG-0pu5q"
   },
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data, backbone):\n",
    "  #data = data.view(batch_size, -1)\n",
    "  with torch.no_grad():\n",
    "    feats = backbone(data)\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "  spike_data = spikegen.rate(feats, num_steps=num_steps, gain=0.95)\n",
    "\n",
    "  spk_out, mem_out, spk1, spk2, spk3 = net(spike_data)\n",
    "\n",
    "  return spk_out, mem_out, spk1, spk2, spk3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "05gb7G2YpqOR"
   },
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_count_loss()\n",
    "num_steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vIiXEObPpi26"
   },
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    spikes1 = 0\n",
    "    spikes2 = 0\n",
    "    spikes3 = 0\n",
    "    spikes_out = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _, spk1, spk2, spk3 = forward_pass(net, num_steps, data, backbone)\n",
    "      spikes_out += torch.sum(spk_rec)\n",
    "      spikes1 += torch.sum(spk1)\n",
    "      spikes2 += torch.sum(spk2)\n",
    "      spikes3 += torch.sum(spk3)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total, spikes1/total, spikes2/total, spikes3/total, spikes_out/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_reset_zero(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden)\n",
    "      init_beta2 = torch.rand(hidden2)\n",
    "      init_beta3 = torch.rand(hidden)\n",
    "      out_beta = torch.rand(output)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, reset_mechanism='zero', learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_reset_subtract(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden)\n",
    "      init_beta2 = torch.rand(hidden2)\n",
    "      init_beta3 = torch.rand(hidden)\n",
    "      out_beta = torch.rand(output)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, reset_mechanism='zero', learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, reset_mechanism='zero', learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_init_weights_low(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden)\n",
    "      init_beta2 = torch.rand(hidden2)\n",
    "      init_beta3 = torch.rand(hidden)\n",
    "      out_beta = torch.rand(output)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "      nn.init.normal_(self.fc1.weight, mean=-0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc2.weight, mean=-0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc3.weight, mean=-0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc4.weight, mean=-0.01, std=0.1)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_init_weights_high(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden)\n",
    "      init_beta2 = torch.rand(hidden2)\n",
    "      init_beta3 = torch.rand(hidden)\n",
    "      out_beta = torch.rand(output)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "      nn.init.normal_(self.fc1.weight, mean=0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc2.weight, mean=0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc3.weight, mean=0.01, std=0.1)\n",
    "      nn.init.normal_(self.fc4.weight, mean=0.01, std=0.1)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_low_initial_beta(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden).div(3)\n",
    "      init_beta2 = torch.rand(hidden2).div(3)\n",
    "      init_beta3 = torch.rand(hidden).div(3)\n",
    "      out_beta = torch.rand(output).div(3)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_medium_initial_beta(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden).div(3).add(1/3)\n",
    "      init_beta2 = torch.rand(hidden2).div(3).add(1/3)\n",
    "      init_beta3 = torch.rand(hidden).div(3).add(1/3)\n",
    "      out_beta = torch.rand(output).div(3).add(1/3)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_high_initial_beta(nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      hidden = 1028\n",
    "      hidden2 = 2048\n",
    "\n",
    "      output = 10\n",
    "\n",
    "      init_beta = torch.rand(hidden).div(3).add(2/3)\n",
    "      init_beta2 = torch.rand(hidden2).div(3).add(2/3)\n",
    "      init_beta3 = torch.rand(hidden).div(3).add(2/3)\n",
    "      out_beta = torch.rand(output).div(3).add(2/3)\n",
    "\n",
    "      # initialize layers\n",
    "      self.fc1 = nn.Linear(512, hidden)\n",
    "      self.lif1 = snn.Leaky(beta=init_beta, learn_beta=True, learn_threshold=True)\n",
    "      self.fc2 = nn.Linear(hidden, hidden2)\n",
    "      self.lif2 = snn.Leaky(beta=init_beta2, learn_beta=True, learn_threshold=True)\n",
    "      self.fc3 = nn.Linear(hidden2, hidden)\n",
    "      self.lif3 = snn.Leaky(beta=init_beta3, learn_beta=True, learn_threshold=True)\n",
    "      self.fc4 = nn.Linear(hidden, output)\n",
    "      self.lif4 = snn.Leaky(beta=out_beta, learn_beta=True, learn_threshold=True, output=True)\n",
    "\n",
    "   def forward(self, x):\n",
    "      mem1 = self.lif1.init_leaky()\n",
    "      mem2 = self.lif2.init_leaky()\n",
    "      mem3 = self.lif3.init_leaky()\n",
    "      mem4 = self.lif4.init_leaky()\n",
    "\n",
    "      spk_out_rec = []  # Record the output trace of spikes\n",
    "      mem_out_rec = []  # Record the output trace of membrane potential\n",
    "      spk1_rec = []\n",
    "      spk2_rec = []\n",
    "      spk3_rec = []\n",
    "      spk4_rec = []\n",
    "\n",
    "      for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            spk_out_rec.append(spk4)\n",
    "            mem_out_rec.append(mem4)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "      return torch.stack(spk_out_rec), torch.stack(mem_out_rec), torch.stack(spk1_rec), torch.stack(spk2_rec), torch.stack(spk3_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hx2e5MAUpXIC"
   },
   "outputs": [],
   "source": [
    "def train(net):\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "  num_epochs = 100\n",
    "  loss_hist = []\n",
    "  test_acc_hist = []\n",
    "  epoch_counter = 0\n",
    "\n",
    "  # Outer training loop\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "      epoch_counter += 1\n",
    "\n",
    "      # Training loop\n",
    "      for data, targets in iter(train_dataloader):\n",
    "          data = data.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          # forward pass\n",
    "          net.train()\n",
    "          spk_rec, _, _, _, _, = forward_pass(net, num_steps, data, backbone)\n",
    "          # initialize the loss & sum over time\n",
    "          loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "          # Gradient calculation + weight update\n",
    "          optimizer.zero_grad()\n",
    "          loss_val.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          # Store loss history for future plotting\n",
    "          loss_hist.append(loss_val.item())\n",
    "\n",
    "          # Test set\n",
    "      if True: #epoch_counter % 10 == 0:\n",
    "          with torch.no_grad():\n",
    "              net.eval()\n",
    "    \n",
    "              # Test set forward pass\n",
    "              test_acc, avg_spikes1, avg_spikes2, avg_spikes3, avg_spikes4 = batch_accuracy(test_dataloader, net, num_steps)\n",
    "              eta = ((time.time() - time_start) / epoch_counter) * (100 - epoch_counter)\n",
    "              print(f\"Epoch {epoch_counter}, Test Acc: {test_acc * 100:.2f}%, eta: {time.strftime('%H:%M:%S', time.gmtime(eta))} \\nAverge spike counts: input layer: {avg_spikes1}, second layer: {avg_spikes2}, third layer: {avg_spikes3}, output layer: {avg_spikes4}\\n\")\n",
    "              \n",
    "              test_acc_hist.append(test_acc.item())\n",
    "\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 9.90%, eta: 05:43:31 \n",
      "Averge spike counts: input layer: 7571.3759765625, second layer: 35912.734375, third layer: 20093.376953125, output layer: 192.2053680419922\n",
      "\n",
      "Epoch 2, Test Acc: 54.28%, eta: 05:40:16 \n",
      "Averge spike counts: input layer: 7464.0625, second layer: 35468.65625, third layer: 20052.248046875, output layer: 135.50558471679688\n",
      "\n",
      "Epoch 3, Test Acc: 66.93%, eta: 05:37:05 \n",
      "Averge spike counts: input layer: 7483.89892578125, second layer: 35304.06640625, third layer: 20020.45703125, output layer: 98.65364837646484\n",
      "\n",
      "Epoch 4, Test Acc: 73.81%, eta: 05:33:19 \n",
      "Averge spike counts: input layer: 7407.7529296875, second layer: 35150.3046875, third layer: 20020.54296875, output layer: 93.04948425292969\n",
      "\n",
      "Epoch 5, Test Acc: 74.48%, eta: 05:30:08 \n",
      "Averge spike counts: input layer: 7378.13427734375, second layer: 35033.84765625, third layer: 20026.54296875, output layer: 90.1573715209961\n",
      "\n",
      "Epoch 6, Test Acc: 74.74%, eta: 05:26:37 \n",
      "Averge spike counts: input layer: 7312.79931640625, second layer: 34883.3125, third layer: 20018.79296875, output layer: 84.72247314453125\n",
      "\n",
      "Epoch 7, Test Acc: 74.29%, eta: 05:23:39 \n",
      "Averge spike counts: input layer: 7325.38623046875, second layer: 34833.0078125, third layer: 20018.697265625, output layer: 78.99107360839844\n",
      "\n",
      "Epoch 8, Test Acc: 77.57%, eta: 05:20:02 \n",
      "Averge spike counts: input layer: 7284.58740234375, second layer: 34713.0234375, third layer: 20010.625, output layer: 75.57775115966797\n",
      "\n",
      "Epoch 9, Test Acc: 75.45%, eta: 05:16:39 \n",
      "Averge spike counts: input layer: 7265.8466796875, second layer: 34617.1015625, third layer: 20018.85546875, output layer: 71.73958587646484\n",
      "\n",
      "Epoch 10, Test Acc: 76.97%, eta: 05:13:02 \n",
      "Averge spike counts: input layer: 7313.30517578125, second layer: 34645.94140625, third layer: 20018.95703125, output layer: 68.40662384033203\n",
      "\n",
      "Epoch 11, Test Acc: 77.05%, eta: 05:09:28 \n",
      "Averge spike counts: input layer: 7276.32958984375, second layer: 34571.28515625, third layer: 20019.685546875, output layer: 66.78646087646484\n",
      "\n",
      "Epoch 12, Test Acc: 76.64%, eta: 05:05:59 \n",
      "Averge spike counts: input layer: 7250.40185546875, second layer: 34504.0703125, third layer: 20020.875, output layer: 61.046875\n",
      "\n",
      "Epoch 13, Test Acc: 78.42%, eta: 05:02:26 \n",
      "Averge spike counts: input layer: 7206.06689453125, second layer: 34408.109375, third layer: 20012.59765625, output layer: 61.45089340209961\n",
      "\n",
      "Epoch 14, Test Acc: 79.80%, eta: 04:58:51 \n",
      "Averge spike counts: input layer: 7112.1845703125, second layer: 34245.9140625, third layer: 20014.892578125, output layer: 57.3508186340332\n",
      "\n",
      "Epoch 15, Test Acc: 79.06%, eta: 04:55:16 \n",
      "Averge spike counts: input layer: 7227.275390625, second layer: 34350.51953125, third layer: 20016.86328125, output layer: 58.001487731933594\n",
      "\n",
      "Epoch 16, Test Acc: 79.46%, eta: 04:51:49 \n",
      "Averge spike counts: input layer: 7119.701171875, second layer: 34180.12109375, third layer: 20021.111328125, output layer: 54.898067474365234\n",
      "\n",
      "Epoch 17, Test Acc: 80.73%, eta: 04:48:15 \n",
      "Averge spike counts: input layer: 7223.9140625, second layer: 34281.27734375, third layer: 20023.935546875, output layer: 54.020835876464844\n",
      "\n",
      "Epoch 18, Test Acc: 80.10%, eta: 04:44:44 \n",
      "Averge spike counts: input layer: 7208.623046875, second layer: 34213.3125, third layer: 20022.66015625, output layer: 54.199405670166016\n",
      "\n",
      "Epoch 19, Test Acc: 80.02%, eta: 04:41:15 \n",
      "Averge spike counts: input layer: 7147.89013671875, second layer: 34146.8828125, third layer: 20022.9140625, output layer: 54.04724884033203\n",
      "\n",
      "Epoch 20, Test Acc: 78.76%, eta: 04:37:46 \n",
      "Averge spike counts: input layer: 7189.75146484375, second layer: 34153.65625, third layer: 20022.375, output layer: 50.21986770629883\n",
      "\n",
      "Epoch 21, Test Acc: 82.18%, eta: 04:34:20 \n",
      "Averge spike counts: input layer: 7149.93310546875, second layer: 34112.37109375, third layer: 20029.521484375, output layer: 54.58073043823242\n",
      "\n",
      "Epoch 22, Test Acc: 80.25%, eta: 04:30:51 \n",
      "Averge spike counts: input layer: 7201.36767578125, second layer: 34142.41796875, third layer: 20022.642578125, output layer: 50.934898376464844\n",
      "\n",
      "Epoch 23, Test Acc: 81.44%, eta: 04:27:21 \n",
      "Averge spike counts: input layer: 7062.7099609375, second layer: 33956.16015625, third layer: 20022.029296875, output layer: 48.735862731933594\n",
      "\n",
      "Epoch 24, Test Acc: 80.73%, eta: 04:23:55 \n",
      "Averge spike counts: input layer: 7174.84375, second layer: 34058.03515625, third layer: 20019.72265625, output layer: 49.875\n",
      "\n",
      "Epoch 25, Test Acc: 80.39%, eta: 04:20:27 \n",
      "Averge spike counts: input layer: 7131.77685546875, second layer: 33963.046875, third layer: 20022.154296875, output layer: 50.1648063659668\n",
      "\n",
      "Epoch 26, Test Acc: 80.54%, eta: 04:16:57 \n",
      "Averge spike counts: input layer: 7212.48974609375, second layer: 34052.13671875, third layer: 20020.81640625, output layer: 48.72172546386719\n",
      "\n",
      "Epoch 27, Test Acc: 80.99%, eta: 04:13:28 \n",
      "Averge spike counts: input layer: 7126.33642578125, second layer: 33923.64453125, third layer: 20016.990234375, output layer: 46.62202453613281\n",
      "\n",
      "Epoch 28, Test Acc: 81.40%, eta: 04:09:59 \n",
      "Averge spike counts: input layer: 7122.822265625, second layer: 33898.58203125, third layer: 20013.46875, output layer: 50.222469329833984\n",
      "\n",
      "Epoch 29, Test Acc: 80.58%, eta: 04:06:31 \n",
      "Averge spike counts: input layer: 7126.46435546875, second layer: 33898.70703125, third layer: 20022.755859375, output layer: 50.24516296386719\n",
      "\n",
      "Epoch 30, Test Acc: 81.25%, eta: 04:03:03 \n",
      "Averge spike counts: input layer: 7109.9697265625, second layer: 33837.4765625, third layer: 20018.41796875, output layer: 49.527156829833984\n",
      "\n",
      "Epoch 31, Test Acc: 83.22%, eta: 03:59:32 \n",
      "Averge spike counts: input layer: 7103.9248046875, second layer: 33828.58984375, third layer: 20016.802734375, output layer: 48.1335563659668\n",
      "\n",
      "Epoch 32, Test Acc: 83.15%, eta: 03:56:03 \n",
      "Averge spike counts: input layer: 7068.86865234375, second layer: 33784.47265625, third layer: 20016.4921875, output layer: 49.33296203613281\n",
      "\n",
      "Epoch 33, Test Acc: 82.63%, eta: 03:52:36 \n",
      "Averge spike counts: input layer: 7076.71142578125, second layer: 33765.7890625, third layer: 20015.947265625, output layer: 47.728050231933594\n",
      "\n",
      "Epoch 34, Test Acc: 81.14%, eta: 03:49:06 \n",
      "Averge spike counts: input layer: 7144.419921875, second layer: 33830.23046875, third layer: 20023.455078125, output layer: 49.49256134033203\n",
      "\n",
      "Epoch 35, Test Acc: 83.30%, eta: 03:45:36 \n",
      "Averge spike counts: input layer: 7151.74853515625, second layer: 33834.671875, third layer: 20020.080078125, output layer: 50.00446701049805\n",
      "\n",
      "Epoch 36, Test Acc: 80.77%, eta: 03:42:07 \n",
      "Averge spike counts: input layer: 7090.0126953125, second layer: 33753.140625, third layer: 20009.791015625, output layer: 48.71131134033203\n",
      "\n",
      "Epoch 37, Test Acc: 82.66%, eta: 03:38:39 \n",
      "Averge spike counts: input layer: 7090.419921875, second layer: 33732.68359375, third layer: 20017.154296875, output layer: 48.97953796386719\n",
      "\n",
      "Epoch 38, Test Acc: 82.70%, eta: 03:35:11 \n",
      "Averge spike counts: input layer: 7074.08203125, second layer: 33703.296875, third layer: 20013.298828125, output layer: 48.21577453613281\n",
      "\n",
      "Epoch 39, Test Acc: 82.44%, eta: 03:31:42 \n",
      "Averge spike counts: input layer: 7065.5263671875, second layer: 33684.04296875, third layer: 20004.45703125, output layer: 48.58370590209961\n",
      "\n",
      "Epoch 40, Test Acc: 83.11%, eta: 03:28:14 \n",
      "Averge spike counts: input layer: 7119.87353515625, second layer: 33730.84375, third layer: 20019.544921875, output layer: 47.9375\n",
      "\n",
      "Epoch 41, Test Acc: 83.33%, eta: 03:24:46 \n",
      "Averge spike counts: input layer: 7102.14013671875, second layer: 33704.96875, third layer: 20019.458984375, output layer: 49.71875\n",
      "\n",
      "Epoch 42, Test Acc: 81.44%, eta: 03:21:19 \n",
      "Averge spike counts: input layer: 7098.8544921875, second layer: 33684.484375, third layer: 20009.8359375, output layer: 50.8046875\n",
      "\n",
      "Epoch 43, Test Acc: 82.37%, eta: 03:17:52 \n",
      "Averge spike counts: input layer: 7074.513671875, second layer: 33634.0, third layer: 20009.796875, output layer: 45.61941909790039\n",
      "\n",
      "Epoch 44, Test Acc: 82.81%, eta: 03:14:23 \n",
      "Averge spike counts: input layer: 7023.67138671875, second layer: 33543.56640625, third layer: 20024.353515625, output layer: 47.140254974365234\n",
      "\n",
      "Epoch 45, Test Acc: 83.48%, eta: 03:10:54 \n",
      "Averge spike counts: input layer: 7018.0537109375, second layer: 33510.671875, third layer: 19999.54296875, output layer: 46.97470474243164\n",
      "\n",
      "Epoch 46, Test Acc: 83.04%, eta: 03:07:26 \n",
      "Averge spike counts: input layer: 7073.6640625, second layer: 33560.18359375, third layer: 20010.681640625, output layer: 48.054317474365234\n",
      "\n",
      "Epoch 47, Test Acc: 83.48%, eta: 03:03:59 \n",
      "Averge spike counts: input layer: 7022.869140625, second layer: 33513.0859375, third layer: 20015.810546875, output layer: 48.915550231933594\n",
      "\n",
      "Epoch 48, Test Acc: 81.70%, eta: 03:00:30 \n",
      "Averge spike counts: input layer: 7043.669921875, second layer: 33497.546875, third layer: 20004.44921875, output layer: 45.5\n",
      "\n",
      "Epoch 49, Test Acc: 83.44%, eta: 02:57:01 \n",
      "Averge spike counts: input layer: 7080.90185546875, second layer: 33564.2109375, third layer: 20006.888671875, output layer: 48.50595474243164\n",
      "\n",
      "Epoch 50, Test Acc: 81.99%, eta: 02:53:34 \n",
      "Averge spike counts: input layer: 7126.15771484375, second layer: 33610.3984375, third layer: 20010.6875, output layer: 50.449031829833984\n",
      "\n",
      "Epoch 51, Test Acc: 83.07%, eta: 02:50:05 \n",
      "Averge spike counts: input layer: 7046.09375, second layer: 33499.71875, third layer: 20006.298828125, output layer: 48.785343170166016\n",
      "\n",
      "Epoch 52, Test Acc: 83.93%, eta: 02:46:36 \n",
      "Averge spike counts: input layer: 7027.50146484375, second layer: 33490.0078125, third layer: 20007.029296875, output layer: 48.17485046386719\n",
      "\n",
      "Epoch 53, Test Acc: 83.82%, eta: 02:43:08 \n",
      "Averge spike counts: input layer: 7102.67724609375, second layer: 33568.86328125, third layer: 20010.98046875, output layer: 47.546504974365234\n",
      "\n",
      "Epoch 54, Test Acc: 83.89%, eta: 02:39:40 \n",
      "Averge spike counts: input layer: 7000.87353515625, second layer: 33426.68359375, third layer: 20021.037109375, output layer: 46.71316909790039\n",
      "\n",
      "Epoch 55, Test Acc: 82.14%, eta: 02:36:11 \n",
      "Averge spike counts: input layer: 7041.62646484375, second layer: 33466.95703125, third layer: 20006.818359375, output layer: 47.75297546386719\n",
      "\n",
      "Epoch 56, Test Acc: 83.07%, eta: 02:32:44 \n",
      "Averge spike counts: input layer: 7042.32373046875, second layer: 33481.984375, third layer: 20021.658203125, output layer: 48.88951110839844\n",
      "\n",
      "Epoch 57, Test Acc: 84.52%, eta: 02:29:15 \n",
      "Averge spike counts: input layer: 7006.17724609375, second layer: 33436.734375, third layer: 20014.279296875, output layer: 49.386905670166016\n",
      "\n",
      "Epoch 58, Test Acc: 83.07%, eta: 02:25:47 \n",
      "Averge spike counts: input layer: 7022.29345703125, second layer: 33444.703125, third layer: 20010.3359375, output layer: 48.677085876464844\n",
      "\n",
      "Epoch 59, Test Acc: 83.85%, eta: 02:22:19 \n",
      "Averge spike counts: input layer: 7042.404296875, second layer: 33452.0078125, third layer: 20016.267578125, output layer: 46.36272430419922\n",
      "\n",
      "Epoch 60, Test Acc: 82.96%, eta: 02:18:51 \n",
      "Averge spike counts: input layer: 7034.35888671875, second layer: 33417.68359375, third layer: 19998.41796875, output layer: 45.33110046386719\n",
      "\n",
      "Epoch 61, Test Acc: 82.89%, eta: 02:15:22 \n",
      "Averge spike counts: input layer: 7016.62060546875, second layer: 33391.484375, third layer: 20005.611328125, output layer: 49.35268020629883\n",
      "\n",
      "Epoch 62, Test Acc: 83.44%, eta: 02:11:54 \n",
      "Averge spike counts: input layer: 7086.2353515625, second layer: 33442.39453125, third layer: 20010.69140625, output layer: 49.42671203613281\n",
      "\n",
      "Epoch 63, Test Acc: 82.78%, eta: 02:08:26 \n",
      "Averge spike counts: input layer: 6945.11328125, second layer: 33266.13671875, third layer: 20000.9140625, output layer: 47.08370590209961\n",
      "\n",
      "Epoch 64, Test Acc: 83.22%, eta: 02:04:57 \n",
      "Averge spike counts: input layer: 7008.22119140625, second layer: 33355.4453125, third layer: 20013.16796875, output layer: 46.86941909790039\n",
      "\n",
      "Epoch 65, Test Acc: 83.67%, eta: 02:01:28 \n",
      "Averge spike counts: input layer: 7004.07080078125, second layer: 33338.890625, third layer: 20010.69140625, output layer: 48.505210876464844\n",
      "\n",
      "Epoch 66, Test Acc: 85.23%, eta: 01:58:00 \n",
      "Averge spike counts: input layer: 7051.380859375, second layer: 33383.66015625, third layer: 20011.673828125, output layer: 47.97098159790039\n",
      "\n",
      "Epoch 67, Test Acc: 83.93%, eta: 01:54:32 \n",
      "Averge spike counts: input layer: 6990.41064453125, second layer: 33271.34765625, third layer: 20010.4765625, output layer: 48.58110046386719\n",
      "\n",
      "Epoch 68, Test Acc: 84.45%, eta: 01:51:04 \n",
      "Averge spike counts: input layer: 6972.96142578125, second layer: 33253.84765625, third layer: 20013.525390625, output layer: 45.91220474243164\n",
      "\n",
      "Epoch 69, Test Acc: 84.34%, eta: 01:47:36 \n",
      "Averge spike counts: input layer: 7000.1591796875, second layer: 33275.6015625, third layer: 20005.19921875, output layer: 47.097843170166016\n",
      "\n",
      "Epoch 70, Test Acc: 83.11%, eta: 01:44:08 \n",
      "Averge spike counts: input layer: 7020.53662109375, second layer: 33300.0546875, third layer: 20001.0546875, output layer: 46.57626724243164\n",
      "\n",
      "Epoch 71, Test Acc: 83.71%, eta: 01:40:39 \n",
      "Averge spike counts: input layer: 6977.12890625, second layer: 33230.19140625, third layer: 20007.421875, output layer: 47.64397430419922\n",
      "\n",
      "Epoch 72, Test Acc: 82.70%, eta: 01:37:11 \n",
      "Averge spike counts: input layer: 6957.3515625, second layer: 33200.1640625, third layer: 20010.548828125, output layer: 45.75260543823242\n",
      "\n",
      "Epoch 73, Test Acc: 84.78%, eta: 01:33:43 \n",
      "Averge spike counts: input layer: 6996.13427734375, second layer: 33239.359375, third layer: 20018.009765625, output layer: 47.39546203613281\n",
      "\n",
      "Epoch 74, Test Acc: 83.26%, eta: 01:30:15 \n",
      "Averge spike counts: input layer: 6985.17431640625, second layer: 33208.8671875, third layer: 20010.66796875, output layer: 47.86421203613281\n",
      "\n",
      "Epoch 75, Test Acc: 84.75%, eta: 01:26:47 \n",
      "Averge spike counts: input layer: 7015.9765625, second layer: 33249.46875, third layer: 20010.716796875, output layer: 49.30282974243164\n",
      "\n",
      "Epoch 76, Test Acc: 83.56%, eta: 01:23:19 \n",
      "Averge spike counts: input layer: 6928.5908203125, second layer: 33156.26953125, third layer: 20009.029296875, output layer: 46.067710876464844\n",
      "\n",
      "Epoch 77, Test Acc: 84.64%, eta: 01:19:50 \n",
      "Averge spike counts: input layer: 7008.8974609375, second layer: 33242.828125, third layer: 19999.88671875, output layer: 44.34040451049805\n",
      "\n",
      "Epoch 78, Test Acc: 84.34%, eta: 01:16:22 \n",
      "Averge spike counts: input layer: 7044.775390625, second layer: 33268.8203125, third layer: 19997.998046875, output layer: 45.61272430419922\n",
      "\n",
      "Epoch 79, Test Acc: 84.11%, eta: 01:12:54 \n",
      "Averge spike counts: input layer: 7009.60205078125, second layer: 33206.37109375, third layer: 20002.470703125, output layer: 43.370906829833984\n",
      "\n",
      "Epoch 80, Test Acc: 84.78%, eta: 01:09:25 \n",
      "Averge spike counts: input layer: 6951.556640625, second layer: 33109.58203125, third layer: 20001.685546875, output layer: 44.66741180419922\n",
      "\n",
      "Epoch 81, Test Acc: 82.92%, eta: 01:05:57 \n",
      "Averge spike counts: input layer: 7091.30224609375, second layer: 33285.78515625, third layer: 20000.859375, output layer: 45.99330520629883\n",
      "\n",
      "Epoch 82, Test Acc: 84.49%, eta: 01:02:29 \n",
      "Averge spike counts: input layer: 7031.2294921875, second layer: 33186.08203125, third layer: 20013.970703125, output layer: 47.528648376464844\n",
      "\n",
      "Epoch 83, Test Acc: 83.67%, eta: 00:59:00 \n",
      "Averge spike counts: input layer: 7015.759765625, second layer: 33177.6953125, third layer: 20008.896484375, output layer: 47.5476188659668\n",
      "\n",
      "Epoch 84, Test Acc: 83.71%, eta: 00:55:32 \n",
      "Averge spike counts: input layer: 7055.76123046875, second layer: 33228.57421875, third layer: 20003.92578125, output layer: 47.629093170166016\n",
      "\n",
      "Epoch 85, Test Acc: 85.64%, eta: 00:52:04 \n",
      "Averge spike counts: input layer: 7062.72265625, second layer: 33221.08203125, third layer: 19998.955078125, output layer: 45.800968170166016\n",
      "\n",
      "Epoch 86, Test Acc: 83.67%, eta: 00:48:36 \n",
      "Averge spike counts: input layer: 6966.97119140625, second layer: 33093.44140625, third layer: 19995.50390625, output layer: 45.265254974365234\n",
      "\n",
      "Epoch 87, Test Acc: 85.04%, eta: 00:45:07 \n",
      "Averge spike counts: input layer: 6996.69970703125, second layer: 33101.85546875, third layer: 19992.369140625, output layer: 46.293155670166016\n",
      "\n",
      "Epoch 88, Test Acc: 84.78%, eta: 00:41:39 \n",
      "Averge spike counts: input layer: 7005.58056640625, second layer: 33122.7890625, third layer: 20006.26171875, output layer: 47.70201110839844\n",
      "\n",
      "Epoch 89, Test Acc: 84.15%, eta: 00:38:11 \n",
      "Averge spike counts: input layer: 6964.4658203125, second layer: 33064.62109375, third layer: 20014.931640625, output layer: 48.101192474365234\n",
      "\n",
      "Epoch 90, Test Acc: 85.23%, eta: 00:34:42 \n",
      "Averge spike counts: input layer: 7014.931640625, second layer: 33106.125, third layer: 20004.533203125, output layer: 43.55580520629883\n",
      "\n",
      "Epoch 91, Test Acc: 83.33%, eta: 00:31:14 \n",
      "Averge spike counts: input layer: 6977.20849609375, second layer: 33048.7578125, third layer: 19999.46875, output layer: 43.47209930419922\n",
      "\n",
      "Epoch 92, Test Acc: 85.27%, eta: 00:27:46 \n",
      "Averge spike counts: input layer: 7009.28564453125, second layer: 33113.01953125, third layer: 20019.48046875, output layer: 44.97470474243164\n",
      "\n",
      "Epoch 93, Test Acc: 84.86%, eta: 00:24:17 \n",
      "Averge spike counts: input layer: 7036.02099609375, second layer: 33128.2578125, third layer: 20014.759765625, output layer: 44.31547546386719\n",
      "\n",
      "Epoch 94, Test Acc: 83.37%, eta: 00:20:49 \n",
      "Averge spike counts: input layer: 7028.44677734375, second layer: 33118.92578125, third layer: 20011.873046875, output layer: 45.70424270629883\n",
      "\n",
      "Epoch 95, Test Acc: 83.18%, eta: 00:17:21 \n",
      "Averge spike counts: input layer: 7013.8857421875, second layer: 33085.859375, third layer: 20015.46484375, output layer: 46.2961311340332\n",
      "\n",
      "Epoch 96, Test Acc: 84.78%, eta: 00:13:53 \n",
      "Averge spike counts: input layer: 7003.1015625, second layer: 33066.5546875, third layer: 20006.10546875, output layer: 46.368675231933594\n",
      "\n",
      "Epoch 97, Test Acc: 84.64%, eta: 00:10:24 \n",
      "Averge spike counts: input layer: 7003.99365234375, second layer: 33039.3671875, third layer: 20006.11328125, output layer: 44.08035659790039\n",
      "\n",
      "Epoch 98, Test Acc: 83.52%, eta: 00:06:56 \n",
      "Averge spike counts: input layer: 6999.9248046875, second layer: 33018.59375, third layer: 19994.970703125, output layer: 44.270835876464844\n",
      "\n",
      "Epoch 99, Test Acc: 83.48%, eta: 00:03:28 \n",
      "Averge spike counts: input layer: 6972.60302734375, second layer: 32992.4296875, third layer: 19999.595703125, output layer: 45.48549270629883\n",
      "\n",
      "Epoch 100, Test Acc: 85.97%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 6983.11474609375, second layer: 33026.171875, third layer: 20009.751953125, output layer: 47.523067474365234\n",
      "\n",
      "20831.65891456604 seconds\n",
      "\n",
      "higher initial weights experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_high_weights = Net_init_weights_high().to(device)\n",
    "net_high_weights = train(net_high_weights)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"higher initial weights experiment\\n\")\n",
    "torch.save(net_high_weights.state_dict(), 'model_high_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 59.75%, eta: 05:48:46 \n",
      "Averge spike counts: input layer: 705.997802734375, second layer: 761.9412231445312, third layer: 353.82330322265625, output layer: 8.216890335083008\n",
      "\n",
      "Epoch 2, Test Acc: 67.89%, eta: 05:44:28 \n",
      "Averge spike counts: input layer: 756.9501342773438, second layer: 837.8154907226562, third layer: 424.0245666503906, output layer: 13.174479484558105\n",
      "\n",
      "Epoch 3, Test Acc: 71.76%, eta: 05:40:00 \n",
      "Averge spike counts: input layer: 776.0718383789062, second layer: 866.4427490234375, third layer: 449.2310485839844, output layer: 15.5505952835083\n",
      "\n",
      "Epoch 4, Test Acc: 72.81%, eta: 05:36:04 \n",
      "Averge spike counts: input layer: 789.7202758789062, second layer: 890.9319458007812, third layer: 467.2206115722656, output layer: 17.309152603149414\n",
      "\n",
      "Epoch 5, Test Acc: 74.70%, eta: 05:32:34 \n",
      "Averge spike counts: input layer: 800.4025268554688, second layer: 904.7444458007812, third layer: 476.7939147949219, output layer: 18.462797164916992\n",
      "\n",
      "Epoch 6, Test Acc: 75.74%, eta: 05:28:45 \n",
      "Averge spike counts: input layer: 807.45947265625, second layer: 912.219482421875, third layer: 481.077392578125, output layer: 19.2113094329834\n",
      "\n",
      "Epoch 7, Test Acc: 75.00%, eta: 05:24:57 \n",
      "Averge spike counts: input layer: 815.1566162109375, second layer: 927.8597412109375, third layer: 488.9605712890625, output layer: 20.20535659790039\n",
      "\n",
      "Epoch 8, Test Acc: 76.45%, eta: 05:21:13 \n",
      "Averge spike counts: input layer: 819.0394287109375, second layer: 935.0349731445312, third layer: 489.15252685546875, output layer: 20.735862731933594\n",
      "\n",
      "Epoch 9, Test Acc: 77.31%, eta: 05:17:33 \n",
      "Averge spike counts: input layer: 828.2183837890625, second layer: 954.9591064453125, third layer: 496.1067810058594, output layer: 21.537202835083008\n",
      "\n",
      "Epoch 10, Test Acc: 76.90%, eta: 05:13:57 \n",
      "Averge spike counts: input layer: 832.3233032226562, second layer: 962.313232421875, third layer: 497.3225402832031, output layer: 21.93303680419922\n",
      "\n",
      "Epoch 11, Test Acc: 78.39%, eta: 05:10:22 \n",
      "Averge spike counts: input layer: 842.2403564453125, second layer: 974.8370971679688, third layer: 506.1689147949219, output layer: 23.069196701049805\n",
      "\n",
      "Epoch 12, Test Acc: 79.69%, eta: 05:06:49 \n",
      "Averge spike counts: input layer: 846.9188842773438, second layer: 986.154052734375, third layer: 509.8575134277344, output layer: 23.675968170166016\n",
      "\n",
      "Epoch 13, Test Acc: 80.02%, eta: 05:03:21 \n",
      "Averge spike counts: input layer: 854.4166870117188, second layer: 999.2593383789062, third layer: 513.7291870117188, output layer: 24.313243865966797\n",
      "\n",
      "Epoch 14, Test Acc: 79.58%, eta: 04:59:48 \n",
      "Averge spike counts: input layer: 863.3787231445312, second layer: 1014.6722412109375, third layer: 520.5792236328125, output layer: 24.891740798950195\n",
      "\n",
      "Epoch 15, Test Acc: 80.32%, eta: 04:56:13 \n",
      "Averge spike counts: input layer: 874.97509765625, second layer: 1029.483642578125, third layer: 526.864990234375, output layer: 26.036087036132812\n",
      "\n",
      "Epoch 16, Test Acc: 79.54%, eta: 04:52:39 \n",
      "Averge spike counts: input layer: 878.953125, second layer: 1035.2637939453125, third layer: 531.3604736328125, output layer: 26.369421005249023\n",
      "\n",
      "Epoch 17, Test Acc: 77.98%, eta: 04:49:05 \n",
      "Averge spike counts: input layer: 887.2109375, second layer: 1049.2850341796875, third layer: 533.1395263671875, output layer: 26.606027603149414\n",
      "\n",
      "Epoch 18, Test Acc: 80.47%, eta: 04:45:38 \n",
      "Averge spike counts: input layer: 895.81884765625, second layer: 1062.9747314453125, third layer: 545.6848754882812, output layer: 27.921875\n",
      "\n",
      "Epoch 19, Test Acc: 81.40%, eta: 04:42:19 \n",
      "Averge spike counts: input layer: 907.872802734375, second layer: 1083.5819091796875, third layer: 553.1130981445312, output layer: 28.853424072265625\n",
      "\n",
      "Epoch 20, Test Acc: 80.36%, eta: 04:38:45 \n",
      "Averge spike counts: input layer: 911.4580078125, second layer: 1091.1722412109375, third layer: 557.7708740234375, output layer: 29.291667938232422\n",
      "\n",
      "Epoch 21, Test Acc: 80.84%, eta: 04:35:14 \n",
      "Averge spike counts: input layer: 918.1693115234375, second layer: 1105.0499267578125, third layer: 561.8057861328125, output layer: 30.248884201049805\n",
      "\n",
      "Epoch 22, Test Acc: 80.10%, eta: 04:31:47 \n",
      "Averge spike counts: input layer: 923.7615356445312, second layer: 1108.46435546875, third layer: 562.8277587890625, output layer: 30.271949768066406\n",
      "\n",
      "Epoch 23, Test Acc: 80.92%, eta: 04:28:20 \n",
      "Averge spike counts: input layer: 929.609375, second layer: 1117.346435546875, third layer: 567.0022583007812, output layer: 30.559152603149414\n",
      "\n",
      "Epoch 24, Test Acc: 81.32%, eta: 04:24:50 \n",
      "Averge spike counts: input layer: 932.7589721679688, second layer: 1124.4349365234375, third layer: 563.1034545898438, output layer: 30.33147430419922\n",
      "\n",
      "Epoch 25, Test Acc: 81.44%, eta: 04:21:17 \n",
      "Averge spike counts: input layer: 942.9855346679688, second layer: 1137.0889892578125, third layer: 569.4453125, output layer: 31.029390335083008\n",
      "\n",
      "Epoch 26, Test Acc: 81.06%, eta: 04:17:50 \n",
      "Averge spike counts: input layer: 947.9721069335938, second layer: 1144.50146484375, third layer: 571.8861694335938, output layer: 31.18489646911621\n",
      "\n",
      "Epoch 27, Test Acc: 81.18%, eta: 04:14:20 \n",
      "Averge spike counts: input layer: 957.9152221679688, second layer: 1159.0592041015625, third layer: 582.7481689453125, output layer: 32.271949768066406\n",
      "\n",
      "Epoch 28, Test Acc: 81.51%, eta: 04:10:48 \n",
      "Averge spike counts: input layer: 960.9642944335938, second layer: 1160.6763916015625, third layer: 582.6090087890625, output layer: 32.43489456176758\n",
      "\n",
      "Epoch 29, Test Acc: 81.29%, eta: 04:07:19 \n",
      "Averge spike counts: input layer: 970.7421875, second layer: 1171.2496337890625, third layer: 583.0822143554688, output layer: 32.57291793823242\n",
      "\n",
      "Epoch 30, Test Acc: 80.88%, eta: 04:03:48 \n",
      "Averge spike counts: input layer: 974.0562133789062, second layer: 1172.40771484375, third layer: 583.2235717773438, output layer: 32.773067474365234\n",
      "\n",
      "Epoch 31, Test Acc: 82.44%, eta: 04:00:20 \n",
      "Averge spike counts: input layer: 984.9910888671875, second layer: 1192.52685546875, third layer: 591.311767578125, output layer: 33.65104293823242\n",
      "\n",
      "Epoch 32, Test Acc: 81.44%, eta: 03:56:51 \n",
      "Averge spike counts: input layer: 996.2567138671875, second layer: 1207.48779296875, third layer: 599.9479370117188, output layer: 34.61830520629883\n",
      "\n",
      "Epoch 33, Test Acc: 82.48%, eta: 03:53:21 \n",
      "Averge spike counts: input layer: 1000.2306518554688, second layer: 1214.629150390625, third layer: 599.586669921875, output layer: 34.756324768066406\n",
      "\n",
      "Epoch 34, Test Acc: 82.14%, eta: 03:49:51 \n",
      "Averge spike counts: input layer: 1008.9635620117188, second layer: 1224.0167236328125, third layer: 603.0278930664062, output layer: 35.00334930419922\n",
      "\n",
      "Epoch 35, Test Acc: 81.36%, eta: 03:46:22 \n",
      "Averge spike counts: input layer: 1014.632080078125, second layer: 1231.2161865234375, third layer: 605.53125, output layer: 35.01860046386719\n",
      "\n",
      "Epoch 36, Test Acc: 82.59%, eta: 03:42:53 \n",
      "Averge spike counts: input layer: 1024.3013916015625, second layer: 1246.0733642578125, third layer: 611.1729736328125, output layer: 35.67633819580078\n",
      "\n",
      "Epoch 37, Test Acc: 81.58%, eta: 03:39:25 \n",
      "Averge spike counts: input layer: 1027.2615966796875, second layer: 1246.830322265625, third layer: 612.313232421875, output layer: 35.70126724243164\n",
      "\n",
      "Epoch 38, Test Acc: 81.92%, eta: 03:35:55 \n",
      "Averge spike counts: input layer: 1031.64404296875, second layer: 1251.1785888671875, third layer: 613.3467407226562, output layer: 36.0241813659668\n",
      "\n",
      "Epoch 39, Test Acc: 81.62%, eta: 03:32:31 \n",
      "Averge spike counts: input layer: 1036.3511962890625, second layer: 1257.9501953125, third layer: 617.9698486328125, output layer: 36.6882438659668\n",
      "\n",
      "Epoch 40, Test Acc: 81.85%, eta: 03:29:01 \n",
      "Averge spike counts: input layer: 1047.367919921875, second layer: 1267.7965087890625, third layer: 621.2224731445312, output layer: 36.644718170166016\n",
      "\n",
      "Epoch 41, Test Acc: 83.37%, eta: 03:25:30 \n",
      "Averge spike counts: input layer: 1053.580322265625, second layer: 1276.1328125, third layer: 622.3385620117188, output layer: 36.53385543823242\n",
      "\n",
      "Epoch 42, Test Acc: 82.18%, eta: 03:22:01 \n",
      "Averge spike counts: input layer: 1054.5546875, second layer: 1277.4888916015625, third layer: 622.8988037109375, output layer: 36.95610046386719\n",
      "\n",
      "Epoch 43, Test Acc: 82.89%, eta: 03:18:31 \n",
      "Averge spike counts: input layer: 1059.07666015625, second layer: 1280.827392578125, third layer: 621.6056518554688, output layer: 36.53608703613281\n",
      "\n",
      "Epoch 44, Test Acc: 82.81%, eta: 03:15:02 \n",
      "Averge spike counts: input layer: 1069.5841064453125, second layer: 1289.63623046875, third layer: 626.16259765625, output layer: 37.03125\n",
      "\n",
      "Epoch 45, Test Acc: 83.07%, eta: 03:11:33 \n",
      "Averge spike counts: input layer: 1075.6551513671875, second layer: 1293.7987060546875, third layer: 624.0889282226562, output layer: 36.3523063659668\n",
      "\n",
      "Epoch 46, Test Acc: 83.11%, eta: 03:08:04 \n",
      "Averge spike counts: input layer: 1085.6395263671875, second layer: 1305.97509765625, third layer: 628.4501342773438, output layer: 36.73883819580078\n",
      "\n",
      "Epoch 47, Test Acc: 82.59%, eta: 03:04:34 \n",
      "Averge spike counts: input layer: 1089.7957763671875, second layer: 1309.466552734375, third layer: 632.5267944335938, output layer: 37.226192474365234\n",
      "\n",
      "Epoch 48, Test Acc: 83.00%, eta: 03:01:07 \n",
      "Averge spike counts: input layer: 1099.8389892578125, second layer: 1321.1175537109375, third layer: 635.4724731445312, output layer: 37.3984375\n",
      "\n",
      "Epoch 49, Test Acc: 83.78%, eta: 02:57:37 \n",
      "Averge spike counts: input layer: 1107.890625, second layer: 1330.765625, third layer: 638.9028930664062, output layer: 38.11049270629883\n",
      "\n",
      "Epoch 50, Test Acc: 82.74%, eta: 02:54:08 \n",
      "Averge spike counts: input layer: 1113.324462890625, second layer: 1336.6168212890625, third layer: 639.1279907226562, output layer: 37.83854293823242\n",
      "\n",
      "Epoch 51, Test Acc: 83.26%, eta: 02:50:38 \n",
      "Averge spike counts: input layer: 1117.10302734375, second layer: 1344.4185791015625, third layer: 635.1651611328125, output layer: 37.571800231933594\n",
      "\n",
      "Epoch 52, Test Acc: 82.92%, eta: 02:47:11 \n",
      "Averge spike counts: input layer: 1125.100830078125, second layer: 1349.630615234375, third layer: 641.1499633789062, output layer: 38.290550231933594\n",
      "\n",
      "Epoch 53, Test Acc: 83.56%, eta: 02:43:42 \n",
      "Averge spike counts: input layer: 1129.9718017578125, second layer: 1353.7362060546875, third layer: 642.328857421875, output layer: 38.1101188659668\n",
      "\n",
      "Epoch 54, Test Acc: 83.26%, eta: 02:40:13 \n",
      "Averge spike counts: input layer: 1136.1484375, second layer: 1364.2410888671875, third layer: 651.0394287109375, output layer: 38.720611572265625\n",
      "\n",
      "Epoch 55, Test Acc: 83.97%, eta: 02:36:45 \n",
      "Averge spike counts: input layer: 1144.049072265625, second layer: 1367.080322265625, third layer: 652.6171875, output layer: 38.855281829833984\n",
      "\n",
      "Epoch 56, Test Acc: 83.93%, eta: 02:33:17 \n",
      "Averge spike counts: input layer: 1150.35498046875, second layer: 1376.46435546875, third layer: 652.882080078125, output layer: 38.511905670166016\n",
      "\n",
      "Epoch 57, Test Acc: 83.93%, eta: 02:29:48 \n",
      "Averge spike counts: input layer: 1156.8057861328125, second layer: 1385.1424560546875, third layer: 656.5952758789062, output layer: 39.28683090209961\n",
      "\n",
      "Epoch 58, Test Acc: 84.38%, eta: 02:26:19 \n",
      "Averge spike counts: input layer: 1155.54248046875, second layer: 1383.174072265625, third layer: 651.0003662109375, output layer: 38.686012268066406\n",
      "\n",
      "Epoch 59, Test Acc: 82.51%, eta: 02:22:49 \n",
      "Averge spike counts: input layer: 1165.682373046875, second layer: 1391.604248046875, third layer: 654.1041870117188, output layer: 38.60268020629883\n",
      "\n",
      "Epoch 60, Test Acc: 83.67%, eta: 02:19:20 \n",
      "Averge spike counts: input layer: 1162.9705810546875, second layer: 1394.1651611328125, third layer: 648.6112670898438, output layer: 38.20424270629883\n",
      "\n",
      "Epoch 61, Test Acc: 83.22%, eta: 02:15:51 \n",
      "Averge spike counts: input layer: 1165.9747314453125, second layer: 1395.7042236328125, third layer: 642.640625, output layer: 37.47135543823242\n",
      "\n",
      "Epoch 62, Test Acc: 84.90%, eta: 02:12:22 \n",
      "Averge spike counts: input layer: 1177.5911865234375, second layer: 1414.9580078125, third layer: 656.2816162109375, output layer: 38.576637268066406\n",
      "\n",
      "Epoch 63, Test Acc: 83.59%, eta: 02:08:53 \n",
      "Averge spike counts: input layer: 1184.1358642578125, second layer: 1423.6629638671875, third layer: 656.928955078125, output layer: 38.81808090209961\n",
      "\n",
      "Epoch 64, Test Acc: 84.75%, eta: 02:05:23 \n",
      "Averge spike counts: input layer: 1191.8408203125, second layer: 1434.29541015625, third layer: 663.5364990234375, output layer: 39.32477569580078\n",
      "\n",
      "Epoch 65, Test Acc: 84.30%, eta: 02:01:55 \n",
      "Averge spike counts: input layer: 1205.33935546875, second layer: 1439.3760986328125, third layer: 670.0599365234375, output layer: 39.66108703613281\n",
      "\n",
      "Epoch 66, Test Acc: 83.63%, eta: 01:58:25 \n",
      "Averge spike counts: input layer: 1203.9661865234375, second layer: 1439.8021240234375, third layer: 665.1380615234375, output layer: 38.593379974365234\n",
      "\n",
      "Epoch 67, Test Acc: 83.71%, eta: 01:54:56 \n",
      "Averge spike counts: input layer: 1213.63427734375, second layer: 1447.7236328125, third layer: 667.4032592773438, output layer: 38.79464340209961\n",
      "\n",
      "Epoch 68, Test Acc: 84.08%, eta: 01:51:27 \n",
      "Averge spike counts: input layer: 1225.988525390625, second layer: 1459.355712890625, third layer: 675.8478393554688, output layer: 39.55022430419922\n",
      "\n",
      "Epoch 69, Test Acc: 84.30%, eta: 01:47:58 \n",
      "Averge spike counts: input layer: 1227.390625, second layer: 1463.97509765625, third layer: 673.7391967773438, output layer: 39.26376724243164\n",
      "\n",
      "Epoch 70, Test Acc: 82.70%, eta: 01:44:29 \n",
      "Averge spike counts: input layer: 1234.158447265625, second layer: 1465.71875, third layer: 675.6964111328125, output layer: 39.58854293823242\n",
      "\n",
      "Epoch 71, Test Acc: 84.15%, eta: 01:41:00 \n",
      "Averge spike counts: input layer: 1230.85791015625, second layer: 1469.5948486328125, third layer: 672.5505981445312, output layer: 38.97172546386719\n",
      "\n",
      "Epoch 72, Test Acc: 84.04%, eta: 01:37:31 \n",
      "Averge spike counts: input layer: 1232.828857421875, second layer: 1467.4918212890625, third layer: 663.755615234375, output layer: 38.23474884033203\n",
      "\n",
      "Epoch 73, Test Acc: 84.30%, eta: 01:34:02 \n",
      "Averge spike counts: input layer: 1243.761962890625, second layer: 1480.84375, third layer: 663.0792236328125, output layer: 38.09263610839844\n",
      "\n",
      "Epoch 74, Test Acc: 82.89%, eta: 01:30:33 \n",
      "Averge spike counts: input layer: 1250.1815185546875, second layer: 1492.04541015625, third layer: 669.3474731445312, output layer: 38.494049072265625\n",
      "\n",
      "Epoch 75, Test Acc: 83.59%, eta: 01:27:04 \n",
      "Averge spike counts: input layer: 1249.7808837890625, second layer: 1493.132080078125, third layer: 669.5725708007812, output layer: 38.51451110839844\n",
      "\n",
      "Epoch 76, Test Acc: 84.04%, eta: 01:23:35 \n",
      "Averge spike counts: input layer: 1261.026123046875, second layer: 1501.5286865234375, third layer: 676.0971069335938, output layer: 38.532737731933594\n",
      "\n",
      "Epoch 77, Test Acc: 83.71%, eta: 01:20:06 \n",
      "Averge spike counts: input layer: 1269.8902587890625, second layer: 1506.0521240234375, third layer: 677.5546875, output layer: 38.321800231933594\n",
      "\n",
      "Epoch 78, Test Acc: 83.78%, eta: 01:16:37 \n",
      "Averge spike counts: input layer: 1273.94873046875, second layer: 1511.3013916015625, third layer: 682.0692138671875, output layer: 38.85268020629883\n",
      "\n",
      "Epoch 79, Test Acc: 84.97%, eta: 01:13:09 \n",
      "Averge spike counts: input layer: 1281.178955078125, second layer: 1514.3043212890625, third layer: 681.5346069335938, output layer: 38.804317474365234\n",
      "\n",
      "Epoch 80, Test Acc: 83.67%, eta: 01:09:40 \n",
      "Averge spike counts: input layer: 1284.2474365234375, second layer: 1515.2705078125, third layer: 684.8229370117188, output layer: 38.868675231933594\n",
      "\n",
      "Epoch 81, Test Acc: 83.30%, eta: 01:06:11 \n",
      "Averge spike counts: input layer: 1296.2738037109375, second layer: 1526.4271240234375, third layer: 685.7269287109375, output layer: 38.865699768066406\n",
      "\n",
      "Epoch 82, Test Acc: 84.34%, eta: 01:02:42 \n",
      "Averge spike counts: input layer: 1301.6328125, second layer: 1532.082275390625, third layer: 693.6893920898438, output layer: 39.62388610839844\n",
      "\n",
      "Epoch 83, Test Acc: 83.56%, eta: 00:59:13 \n",
      "Averge spike counts: input layer: 1304.8582763671875, second layer: 1530.6488037109375, third layer: 688.380615234375, output layer: 38.70349884033203\n",
      "\n",
      "Epoch 84, Test Acc: 84.97%, eta: 00:55:44 \n",
      "Averge spike counts: input layer: 1308.7225341796875, second layer: 1530.1112060546875, third layer: 685.544677734375, output layer: 38.30915069580078\n",
      "\n",
      "Epoch 85, Test Acc: 84.56%, eta: 00:52:15 \n",
      "Averge spike counts: input layer: 1309.996337890625, second layer: 1533.033935546875, third layer: 687.2931518554688, output layer: 38.77120590209961\n",
      "\n",
      "Epoch 86, Test Acc: 82.96%, eta: 00:48:46 \n",
      "Averge spike counts: input layer: 1317.0096435546875, second layer: 1543.5408935546875, third layer: 690.8995361328125, output layer: 38.7961311340332\n",
      "\n",
      "Epoch 87, Test Acc: 83.85%, eta: 00:45:17 \n",
      "Averge spike counts: input layer: 1320.5762939453125, second layer: 1551.05029296875, third layer: 698.1409912109375, output layer: 39.14099884033203\n",
      "\n",
      "Epoch 88, Test Acc: 85.01%, eta: 00:41:47 \n",
      "Averge spike counts: input layer: 1321.863525390625, second layer: 1558.114990234375, third layer: 694.0859375, output layer: 39.165550231933594\n",
      "\n",
      "Epoch 89, Test Acc: 84.97%, eta: 00:38:18 \n",
      "Averge spike counts: input layer: 1328.7537841796875, second layer: 1556.6593017578125, third layer: 693.35009765625, output layer: 38.65327453613281\n",
      "\n",
      "Epoch 90, Test Acc: 84.82%, eta: 00:34:49 \n",
      "Averge spike counts: input layer: 1331.3519287109375, second layer: 1568.2943115234375, third layer: 696.850830078125, output layer: 39.05506134033203\n",
      "\n",
      "Epoch 91, Test Acc: 84.45%, eta: 00:31:20 \n",
      "Averge spike counts: input layer: 1334.987060546875, second layer: 1573.4207763671875, third layer: 695.1651611328125, output layer: 38.97433090209961\n",
      "\n",
      "Epoch 92, Test Acc: 86.01%, eta: 00:27:51 \n",
      "Averge spike counts: input layer: 1342.73291015625, second layer: 1577.8895263671875, third layer: 698.5253295898438, output layer: 39.12202453613281\n",
      "\n",
      "Epoch 93, Test Acc: 84.41%, eta: 00:24:22 \n",
      "Averge spike counts: input layer: 1348.0770263671875, second layer: 1586.591552734375, third layer: 704.500732421875, output layer: 39.32366180419922\n",
      "\n",
      "Epoch 94, Test Acc: 84.08%, eta: 00:20:53 \n",
      "Averge spike counts: input layer: 1354.0252685546875, second layer: 1586.031982421875, third layer: 702.4903564453125, output layer: 38.662574768066406\n",
      "\n",
      "Epoch 95, Test Acc: 84.38%, eta: 00:17:24 \n",
      "Averge spike counts: input layer: 1354.603759765625, second layer: 1588.6864013671875, third layer: 702.234375, output layer: 38.90178680419922\n",
      "\n",
      "Epoch 96, Test Acc: 84.60%, eta: 00:13:55 \n",
      "Averge spike counts: input layer: 1359.5386962890625, second layer: 1595.0479736328125, third layer: 704.1409912109375, output layer: 38.71316909790039\n",
      "\n",
      "Epoch 97, Test Acc: 84.78%, eta: 00:10:26 \n",
      "Averge spike counts: input layer: 1365.622802734375, second layer: 1596.99365234375, third layer: 705.3456420898438, output layer: 38.110862731933594\n",
      "\n",
      "Epoch 98, Test Acc: 84.71%, eta: 00:06:57 \n",
      "Averge spike counts: input layer: 1375.052490234375, second layer: 1603.1380615234375, third layer: 707.5889282226562, output layer: 38.446800231933594\n",
      "\n",
      "Epoch 99, Test Acc: 84.64%, eta: 00:03:28 \n",
      "Averge spike counts: input layer: 1375.71728515625, second layer: 1603.807373046875, third layer: 704.4761962890625, output layer: 38.412574768066406\n",
      "\n",
      "Epoch 100, Test Acc: 84.67%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 1376.9542236328125, second layer: 1612.461669921875, third layer: 706.3966064453125, output layer: 38.22991180419922\n",
      "\n",
      "20895.932761907578 seconds\n",
      "\n",
      "lower initial weights experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_low_weights = Net_init_weights_low().to(device)\n",
    "net_low_weights = train(net_low_weights)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"lower initial weights experiment\\n\")\n",
    "torch.save(net_low_weights.state_dict(), 'model_low_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 28.57%, eta: 05:43:41 \n",
      "Averge spike counts: input layer: 1641.5401611328125, second layer: 2123.08154296875, third layer: 413.6499328613281, output layer: 6.1432294845581055\n",
      "\n",
      "Epoch 2, Test Acc: 51.38%, eta: 05:41:26 \n",
      "Averge spike counts: input layer: 2351.525390625, second layer: 4091.930419921875, third layer: 928.2191162109375, output layer: 9.114583969116211\n",
      "\n",
      "Epoch 3, Test Acc: 59.00%, eta: 05:37:56 \n",
      "Averge spike counts: input layer: 2822.415283203125, second layer: 5500.6826171875, third layer: 1341.43896484375, output layer: 18.702009201049805\n",
      "\n",
      "Epoch 4, Test Acc: 67.41%, eta: 05:34:34 \n",
      "Averge spike counts: input layer: 3276.00537109375, second layer: 6808.09228515625, third layer: 1571.7738037109375, output layer: 21.678199768066406\n",
      "\n",
      "Epoch 5, Test Acc: 72.99%, eta: 05:31:03 \n",
      "Averge spike counts: input layer: 3641.80810546875, second layer: 7959.7412109375, third layer: 1759.474365234375, output layer: 21.009300231933594\n",
      "\n",
      "Epoch 6, Test Acc: 73.18%, eta: 05:27:22 \n",
      "Averge spike counts: input layer: 3778.24365234375, second layer: 8487.150390625, third layer: 1873.79541015625, output layer: 22.26860237121582\n",
      "\n",
      "Epoch 7, Test Acc: 75.67%, eta: 05:23:54 \n",
      "Averge spike counts: input layer: 3999.506103515625, second layer: 9149.6806640625, third layer: 1995.4832763671875, output layer: 23.760046005249023\n",
      "\n",
      "Epoch 8, Test Acc: 77.01%, eta: 05:20:38 \n",
      "Averge spike counts: input layer: 4094.77978515625, second layer: 9468.6005859375, third layer: 2066.4833984375, output layer: 24.37053680419922\n",
      "\n",
      "Epoch 9, Test Acc: 77.12%, eta: 05:17:02 \n",
      "Averge spike counts: input layer: 4232.05419921875, second layer: 9873.8232421875, third layer: 2163.87646484375, output layer: 25.94084930419922\n",
      "\n",
      "Epoch 10, Test Acc: 79.17%, eta: 05:13:28 \n",
      "Averge spike counts: input layer: 4291.19921875, second layer: 10093.126953125, third layer: 2176.958251953125, output layer: 25.760046005249023\n",
      "\n",
      "Epoch 11, Test Acc: 77.86%, eta: 05:10:28 \n",
      "Averge spike counts: input layer: 4355.083984375, second layer: 10264.119140625, third layer: 2196.994873046875, output layer: 25.66927146911621\n",
      "\n",
      "Epoch 12, Test Acc: 80.47%, eta: 05:06:50 \n",
      "Averge spike counts: input layer: 4538.916015625, second layer: 10741.54296875, third layer: 2281.4658203125, output layer: 27.01860237121582\n",
      "\n",
      "Epoch 13, Test Acc: 79.58%, eta: 05:03:23 \n",
      "Averge spike counts: input layer: 4555.9716796875, second layer: 10841.099609375, third layer: 2274.03466796875, output layer: 26.31026840209961\n",
      "\n",
      "Epoch 14, Test Acc: 78.98%, eta: 04:59:50 \n",
      "Averge spike counts: input layer: 4603.73681640625, second layer: 10923.0654296875, third layer: 2336.586669921875, output layer: 28.403274536132812\n",
      "\n",
      "Epoch 15, Test Acc: 80.10%, eta: 04:56:16 \n",
      "Averge spike counts: input layer: 4640.11474609375, second layer: 10988.0927734375, third layer: 2393.913818359375, output layer: 29.663318634033203\n",
      "\n",
      "Epoch 16, Test Acc: 79.87%, eta: 04:52:51 \n",
      "Averge spike counts: input layer: 4667.5546875, second layer: 11040.1103515625, third layer: 2410.719482421875, output layer: 30.19828987121582\n",
      "\n",
      "Epoch 17, Test Acc: 80.92%, eta: 04:49:24 \n",
      "Averge spike counts: input layer: 4749.1572265625, second layer: 11234.927734375, third layer: 2427.7490234375, output layer: 30.256696701049805\n",
      "\n",
      "Epoch 18, Test Acc: 80.10%, eta: 04:45:51 \n",
      "Averge spike counts: input layer: 4795.72021484375, second layer: 11333.8623046875, third layer: 2454.427978515625, output layer: 30.306175231933594\n",
      "\n",
      "Epoch 19, Test Acc: 80.54%, eta: 04:42:16 \n",
      "Averge spike counts: input layer: 4841.33251953125, second layer: 11421.365234375, third layer: 2466.2890625, output layer: 31.02901840209961\n",
      "\n",
      "Epoch 20, Test Acc: 81.14%, eta: 04:38:46 \n",
      "Averge spike counts: input layer: 4846.82958984375, second layer: 11460.08984375, third layer: 2473.616455078125, output layer: 31.699405670166016\n",
      "\n",
      "Epoch 21, Test Acc: 81.44%, eta: 04:35:15 \n",
      "Averge spike counts: input layer: 4905.22119140625, second layer: 11551.8466796875, third layer: 2530.897705078125, output layer: 32.45573043823242\n",
      "\n",
      "Epoch 22, Test Acc: 82.33%, eta: 04:31:46 \n",
      "Averge spike counts: input layer: 4920.33251953125, second layer: 11573.1513671875, third layer: 2518.599853515625, output layer: 32.45089340209961\n",
      "\n",
      "Epoch 23, Test Acc: 83.30%, eta: 04:28:15 \n",
      "Averge spike counts: input layer: 4927.1123046875, second layer: 11541.873046875, third layer: 2481.339599609375, output layer: 31.946056365966797\n",
      "\n",
      "Epoch 24, Test Acc: 82.63%, eta: 04:24:44 \n",
      "Averge spike counts: input layer: 5033.81396484375, second layer: 11727.1982421875, third layer: 2537.855712890625, output layer: 32.95573043823242\n",
      "\n",
      "Epoch 25, Test Acc: 82.81%, eta: 04:21:14 \n",
      "Averge spike counts: input layer: 5021.08740234375, second layer: 11639.46484375, third layer: 2532.578857421875, output layer: 33.51301956176758\n",
      "\n",
      "Epoch 26, Test Acc: 81.51%, eta: 04:17:48 \n",
      "Averge spike counts: input layer: 5042.96826171875, second layer: 11673.138671875, third layer: 2521.203125, output layer: 33.07477569580078\n",
      "\n",
      "Epoch 27, Test Acc: 81.66%, eta: 04:14:20 \n",
      "Averge spike counts: input layer: 5068.453125, second layer: 11685.44921875, third layer: 2530.435302734375, output layer: 32.66889953613281\n",
      "\n",
      "Epoch 28, Test Acc: 80.84%, eta: 04:10:48 \n",
      "Averge spike counts: input layer: 5174.12109375, second layer: 11834.5556640625, third layer: 2527.19970703125, output layer: 33.89620590209961\n",
      "\n",
      "Epoch 29, Test Acc: 82.85%, eta: 04:07:19 \n",
      "Averge spike counts: input layer: 5098.95361328125, second layer: 11671.9873046875, third layer: 2572.84375, output layer: 34.29724884033203\n",
      "\n",
      "Epoch 30, Test Acc: 83.18%, eta: 04:03:48 \n",
      "Averge spike counts: input layer: 5191.40478515625, second layer: 11842.18359375, third layer: 2599.74267578125, output layer: 34.57514953613281\n",
      "\n",
      "Epoch 31, Test Acc: 83.41%, eta: 04:00:18 \n",
      "Averge spike counts: input layer: 5210.4794921875, second layer: 11862.908203125, third layer: 2565.196533203125, output layer: 34.17299270629883\n",
      "\n",
      "Epoch 32, Test Acc: 83.78%, eta: 03:56:48 \n",
      "Averge spike counts: input layer: 5156.36962890625, second layer: 11776.31640625, third layer: 2563.4755859375, output layer: 35.749629974365234\n",
      "\n",
      "Epoch 33, Test Acc: 83.18%, eta: 03:53:17 \n",
      "Averge spike counts: input layer: 5201.90283203125, second layer: 11817.443359375, third layer: 2531.127197265625, output layer: 33.74702453613281\n",
      "\n",
      "Epoch 34, Test Acc: 83.07%, eta: 03:49:50 \n",
      "Averge spike counts: input layer: 5211.68505859375, second layer: 11824.4248046875, third layer: 2557.293212890625, output layer: 35.36979293823242\n",
      "\n",
      "Epoch 35, Test Acc: 82.81%, eta: 03:46:20 \n",
      "Averge spike counts: input layer: 5191.12060546875, second layer: 11766.451171875, third layer: 2546.158935546875, output layer: 34.998512268066406\n",
      "\n",
      "Epoch 36, Test Acc: 82.48%, eta: 03:42:51 \n",
      "Averge spike counts: input layer: 5292.458984375, second layer: 11919.4736328125, third layer: 2580.82861328125, output layer: 35.74107360839844\n",
      "\n",
      "Epoch 37, Test Acc: 82.63%, eta: 03:39:21 \n",
      "Averge spike counts: input layer: 5299.1875, second layer: 11902.5087890625, third layer: 2606.04541015625, output layer: 35.89099884033203\n",
      "\n",
      "Epoch 38, Test Acc: 83.71%, eta: 03:35:52 \n",
      "Averge spike counts: input layer: 5234.349609375, second layer: 11775.197265625, third layer: 2565.488037109375, output layer: 36.161460876464844\n",
      "\n",
      "Epoch 39, Test Acc: 83.97%, eta: 03:32:24 \n",
      "Averge spike counts: input layer: 5303.166015625, second layer: 11829.865234375, third layer: 2612.39892578125, output layer: 36.584449768066406\n",
      "\n",
      "Epoch 40, Test Acc: 83.41%, eta: 03:28:54 \n",
      "Averge spike counts: input layer: 5283.4619140625, second layer: 11758.4111328125, third layer: 2583.3173828125, output layer: 36.431175231933594\n",
      "\n",
      "Epoch 41, Test Acc: 83.18%, eta: 03:25:24 \n",
      "Averge spike counts: input layer: 5348.88330078125, second layer: 11832.9990234375, third layer: 2580.647705078125, output layer: 36.52455520629883\n",
      "\n",
      "Epoch 42, Test Acc: 84.41%, eta: 03:21:55 \n",
      "Averge spike counts: input layer: 5330.5673828125, second layer: 11746.7939453125, third layer: 2586.328125, output layer: 35.7741813659668\n",
      "\n",
      "Epoch 43, Test Acc: 83.33%, eta: 03:18:28 \n",
      "Averge spike counts: input layer: 5276.328125, second layer: 11664.052734375, third layer: 2573.83349609375, output layer: 36.56547546386719\n",
      "\n",
      "Epoch 44, Test Acc: 84.00%, eta: 03:14:58 \n",
      "Averge spike counts: input layer: 5337.150390625, second layer: 11689.412109375, third layer: 2565.20166015625, output layer: 35.51599884033203\n",
      "\n",
      "Epoch 45, Test Acc: 84.26%, eta: 03:11:31 \n",
      "Averge spike counts: input layer: 5343.6728515625, second layer: 11695.6357421875, third layer: 2570.962158203125, output layer: 36.94791793823242\n",
      "\n",
      "Epoch 46, Test Acc: 83.30%, eta: 03:08:02 \n",
      "Averge spike counts: input layer: 5273.6513671875, second layer: 11560.65625, third layer: 2592.66845703125, output layer: 36.0078125\n",
      "\n",
      "Epoch 47, Test Acc: 83.15%, eta: 03:04:37 \n",
      "Averge spike counts: input layer: 5257.96240234375, second layer: 11575.833984375, third layer: 2569.24365234375, output layer: 37.51227569580078\n",
      "\n",
      "Epoch 48, Test Acc: 82.89%, eta: 03:01:09 \n",
      "Averge spike counts: input layer: 5387.142578125, second layer: 11746.658203125, third layer: 2563.58935546875, output layer: 37.53683090209961\n",
      "\n",
      "Epoch 49, Test Acc: 84.49%, eta: 02:57:39 \n",
      "Averge spike counts: input layer: 5272.90234375, second layer: 11536.1220703125, third layer: 2525.869384765625, output layer: 36.582218170166016\n",
      "\n",
      "Epoch 50, Test Acc: 83.52%, eta: 02:54:10 \n",
      "Averge spike counts: input layer: 5305.4267578125, second layer: 11540.474609375, third layer: 2550.860107421875, output layer: 36.95870590209961\n",
      "\n",
      "Epoch 51, Test Acc: 84.78%, eta: 02:50:42 \n",
      "Averge spike counts: input layer: 5351.9609375, second layer: 11584.9990234375, third layer: 2553.298095703125, output layer: 36.839656829833984\n",
      "\n",
      "Epoch 52, Test Acc: 84.41%, eta: 02:47:13 \n",
      "Averge spike counts: input layer: 5317.71484375, second layer: 11565.3408203125, third layer: 2533.24267578125, output layer: 36.45535659790039\n",
      "\n",
      "Epoch 53, Test Acc: 83.89%, eta: 02:43:43 \n",
      "Averge spike counts: input layer: 5351.701171875, second layer: 11556.865234375, third layer: 2541.441162109375, output layer: 37.871280670166016\n",
      "\n",
      "Epoch 54, Test Acc: 84.38%, eta: 02:40:14 \n",
      "Averge spike counts: input layer: 5301.85986328125, second layer: 11491.5625, third layer: 2553.943603515625, output layer: 37.624629974365234\n",
      "\n",
      "Epoch 55, Test Acc: 84.23%, eta: 02:36:45 \n",
      "Averge spike counts: input layer: 5347.548828125, second layer: 11526.4599609375, third layer: 2592.900634765625, output layer: 38.408111572265625\n",
      "\n",
      "Epoch 56, Test Acc: 84.15%, eta: 02:33:15 \n",
      "Averge spike counts: input layer: 5360.55615234375, second layer: 11500.6923828125, third layer: 2621.3984375, output layer: 38.355281829833984\n",
      "\n",
      "Epoch 57, Test Acc: 84.19%, eta: 02:29:46 \n",
      "Averge spike counts: input layer: 5397.6259765625, second layer: 11551.51953125, third layer: 2608.392822265625, output layer: 38.364585876464844\n",
      "\n",
      "Epoch 58, Test Acc: 84.04%, eta: 02:26:16 \n",
      "Averge spike counts: input layer: 5397.92236328125, second layer: 11553.857421875, third layer: 2581.96435546875, output layer: 37.36941909790039\n",
      "\n",
      "Epoch 59, Test Acc: 83.52%, eta: 02:22:47 \n",
      "Averge spike counts: input layer: 5358.56201171875, second layer: 11463.0625, third layer: 2574.4501953125, output layer: 38.456844329833984\n",
      "\n",
      "Epoch 60, Test Acc: 84.49%, eta: 02:19:20 \n",
      "Averge spike counts: input layer: 5317.8759765625, second layer: 11362.939453125, third layer: 2547.873291015625, output layer: 38.88951110839844\n",
      "\n",
      "Epoch 61, Test Acc: 85.16%, eta: 02:15:51 \n",
      "Averge spike counts: input layer: 5273.30712890625, second layer: 11270.9765625, third layer: 2598.91162109375, output layer: 39.62165069580078\n",
      "\n",
      "Epoch 62, Test Acc: 85.01%, eta: 02:12:21 \n",
      "Averge spike counts: input layer: 5347.634765625, second layer: 11414.810546875, third layer: 2571.237060546875, output layer: 38.70349884033203\n",
      "\n",
      "Epoch 63, Test Acc: 85.12%, eta: 02:08:52 \n",
      "Averge spike counts: input layer: 5314.76904296875, second layer: 11309.3935546875, third layer: 2549.1767578125, output layer: 38.17448043823242\n",
      "\n",
      "Epoch 64, Test Acc: 83.85%, eta: 02:05:23 \n",
      "Averge spike counts: input layer: 5370.4736328125, second layer: 11422.6494140625, third layer: 2546.627685546875, output layer: 38.17224884033203\n",
      "\n",
      "Epoch 65, Test Acc: 84.56%, eta: 02:01:54 \n",
      "Averge spike counts: input layer: 5311.728515625, second layer: 11327.1181640625, third layer: 2559.38427734375, output layer: 39.402530670166016\n",
      "\n",
      "Epoch 66, Test Acc: 84.00%, eta: 01:58:25 \n",
      "Averge spike counts: input layer: 5260.1357421875, second layer: 11208.0400390625, third layer: 2568.55322265625, output layer: 38.24441909790039\n",
      "\n",
      "Epoch 67, Test Acc: 85.27%, eta: 01:54:55 \n",
      "Averge spike counts: input layer: 5304.0791015625, second layer: 11249.51171875, third layer: 2554.0546875, output layer: 37.76041793823242\n",
      "\n",
      "Epoch 68, Test Acc: 83.15%, eta: 01:51:26 \n",
      "Averge spike counts: input layer: 5302.5732421875, second layer: 11266.69921875, third layer: 2543.55029296875, output layer: 38.96428680419922\n",
      "\n",
      "Epoch 69, Test Acc: 85.71%, eta: 01:47:57 \n",
      "Averge spike counts: input layer: 5383.5634765625, second layer: 11387.677734375, third layer: 2521.338134765625, output layer: 38.79948043823242\n",
      "\n",
      "Epoch 70, Test Acc: 84.41%, eta: 01:44:28 \n",
      "Averge spike counts: input layer: 5319.8349609375, second layer: 11284.7275390625, third layer: 2559.523193359375, output layer: 39.403648376464844\n",
      "\n",
      "Epoch 71, Test Acc: 84.82%, eta: 01:40:59 \n",
      "Averge spike counts: input layer: 5316.31298828125, second layer: 11234.8837890625, third layer: 2547.470947265625, output layer: 37.923362731933594\n",
      "\n",
      "Epoch 72, Test Acc: 84.26%, eta: 01:37:29 \n",
      "Averge spike counts: input layer: 5297.49951171875, second layer: 11192.701171875, third layer: 2537.944580078125, output layer: 39.44754409790039\n",
      "\n",
      "Epoch 73, Test Acc: 84.86%, eta: 01:34:00 \n",
      "Averge spike counts: input layer: 5280.86865234375, second layer: 11173.115234375, third layer: 2557.13037109375, output layer: 39.14546203613281\n",
      "\n",
      "Epoch 74, Test Acc: 84.52%, eta: 01:30:32 \n",
      "Averge spike counts: input layer: 5321.67626953125, second layer: 11186.521484375, third layer: 2537.425537109375, output layer: 38.36049270629883\n",
      "\n",
      "Epoch 75, Test Acc: 85.71%, eta: 01:27:03 \n",
      "Averge spike counts: input layer: 5283.8720703125, second layer: 11136.037109375, third layer: 2545.710205078125, output layer: 38.79724884033203\n",
      "\n",
      "Epoch 76, Test Acc: 84.15%, eta: 01:23:34 \n",
      "Averge spike counts: input layer: 5288.15673828125, second layer: 11119.08984375, third layer: 2510.7548828125, output layer: 38.025299072265625\n",
      "\n",
      "Epoch 77, Test Acc: 84.38%, eta: 01:20:05 \n",
      "Averge spike counts: input layer: 5220.32763671875, second layer: 10977.708984375, third layer: 2506.00830078125, output layer: 37.54464340209961\n",
      "\n",
      "Epoch 78, Test Acc: 84.67%, eta: 01:16:36 \n",
      "Averge spike counts: input layer: 5250.4716796875, second layer: 11058.125, third layer: 2507.196044921875, output layer: 38.872398376464844\n",
      "\n",
      "Epoch 79, Test Acc: 85.45%, eta: 01:13:07 \n",
      "Averge spike counts: input layer: 5341.35546875, second layer: 11177.7587890625, third layer: 2510.72802734375, output layer: 38.54910659790039\n",
      "\n",
      "Epoch 80, Test Acc: 85.12%, eta: 01:09:38 \n",
      "Averge spike counts: input layer: 5257.55908203125, second layer: 11062.7451171875, third layer: 2514.068115234375, output layer: 40.2741813659668\n",
      "\n",
      "Epoch 81, Test Acc: 84.78%, eta: 01:06:09 \n",
      "Averge spike counts: input layer: 5245.64111328125, second layer: 11050.1044921875, third layer: 2531.81689453125, output layer: 39.16071319580078\n",
      "\n",
      "Epoch 82, Test Acc: 84.71%, eta: 01:02:40 \n",
      "Averge spike counts: input layer: 5300.2822265625, second layer: 11055.1923828125, third layer: 2547.138916015625, output layer: 39.09151840209961\n",
      "\n",
      "Epoch 83, Test Acc: 85.04%, eta: 00:59:11 \n",
      "Averge spike counts: input layer: 5252.1220703125, second layer: 10989.5556640625, third layer: 2523.57080078125, output layer: 38.23214340209961\n",
      "\n",
      "Epoch 84, Test Acc: 84.34%, eta: 00:55:42 \n",
      "Averge spike counts: input layer: 5221.79638671875, second layer: 10938.626953125, third layer: 2485.797607421875, output layer: 38.6242561340332\n",
      "\n",
      "Epoch 85, Test Acc: 83.78%, eta: 00:52:14 \n",
      "Averge spike counts: input layer: 5253.7529296875, second layer: 10922.482421875, third layer: 2503.08740234375, output layer: 38.691593170166016\n",
      "\n",
      "Epoch 86, Test Acc: 84.97%, eta: 00:48:47 \n",
      "Averge spike counts: input layer: 5300.828125, second layer: 10932.7919921875, third layer: 2501.666259765625, output layer: 37.204986572265625\n",
      "\n",
      "Epoch 87, Test Acc: 85.57%, eta: 00:45:19 \n",
      "Averge spike counts: input layer: 5175.505859375, second layer: 10839.51171875, third layer: 2484.571044921875, output layer: 37.890625\n",
      "\n",
      "Epoch 88, Test Acc: 84.60%, eta: 00:41:50 \n",
      "Averge spike counts: input layer: 5211.7080078125, second layer: 10860.7158203125, third layer: 2493.939697265625, output layer: 37.902156829833984\n",
      "\n",
      "Epoch 89, Test Acc: 84.67%, eta: 00:38:20 \n",
      "Averge spike counts: input layer: 5211.28173828125, second layer: 10863.1181640625, third layer: 2506.5859375, output layer: 37.99888610839844\n",
      "\n",
      "Epoch 90, Test Acc: 85.75%, eta: 00:34:51 \n",
      "Averge spike counts: input layer: 5228.16845703125, second layer: 10786.498046875, third layer: 2516.89208984375, output layer: 38.27790069580078\n",
      "\n",
      "Epoch 91, Test Acc: 84.45%, eta: 00:31:22 \n",
      "Averge spike counts: input layer: 5260.24560546875, second layer: 10852.4306640625, third layer: 2510.884765625, output layer: 38.934898376464844\n",
      "\n",
      "Epoch 92, Test Acc: 86.01%, eta: 00:27:53 \n",
      "Averge spike counts: input layer: 5180.0888671875, second layer: 10771.74609375, third layer: 2505.311767578125, output layer: 39.246280670166016\n",
      "\n",
      "Epoch 93, Test Acc: 85.34%, eta: 00:24:24 \n",
      "Averge spike counts: input layer: 5214.67138671875, second layer: 10844.3583984375, third layer: 2507.2958984375, output layer: 38.7023811340332\n",
      "\n",
      "Epoch 94, Test Acc: 84.93%, eta: 00:20:54 \n",
      "Averge spike counts: input layer: 5184.15087890625, second layer: 10774.58203125, third layer: 2517.541259765625, output layer: 38.32143020629883\n",
      "\n",
      "Epoch 95, Test Acc: 85.45%, eta: 00:17:25 \n",
      "Averge spike counts: input layer: 5142.77490234375, second layer: 10710.990234375, third layer: 2497.687255859375, output layer: 38.16294860839844\n",
      "\n",
      "Epoch 96, Test Acc: 84.45%, eta: 00:13:56 \n",
      "Averge spike counts: input layer: 5157.4228515625, second layer: 10768.140625, third layer: 2479.828125, output layer: 38.804317474365234\n",
      "\n",
      "Epoch 97, Test Acc: 85.86%, eta: 00:10:27 \n",
      "Averge spike counts: input layer: 5109.61669921875, second layer: 10644.255859375, third layer: 2504.997802734375, output layer: 38.78720474243164\n",
      "\n",
      "Epoch 98, Test Acc: 85.68%, eta: 00:06:58 \n",
      "Averge spike counts: input layer: 5148.51611328125, second layer: 10704.7783203125, third layer: 2473.41162109375, output layer: 39.21316909790039\n",
      "\n",
      "Epoch 99, Test Acc: 86.57%, eta: 00:03:29 \n",
      "Averge spike counts: input layer: 5123.50390625, second layer: 10671.5986328125, third layer: 2500.4287109375, output layer: 38.753719329833984\n",
      "\n",
      "Epoch 100, Test Acc: 85.38%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 5144.80322265625, second layer: 10681.259765625, third layer: 2505.611328125, output layer: 39.96354293823242\n",
      "\n",
      "20916.20783495903 seconds\n",
      "\n",
      "lower initial beta experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_low_beta = Net_low_initial_beta().to(device)\n",
    "net_low_beta = train(net_low_beta)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"lower initial beta experiment\\n\")\n",
    "torch.save(net_low_beta.state_dict(), 'model_low_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NXyEqbnlbJ2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 45.35%, eta: 05:44:47 \n",
      "Averge spike counts: input layer: 2326.15283203125, second layer: 3710.144775390625, third layer: 893.9390258789062, output layer: 9.1056547164917\n",
      "\n",
      "Epoch 2, Test Acc: 62.28%, eta: 05:41:02 \n",
      "Averge spike counts: input layer: 3138.989990234375, second layer: 6428.01416015625, third layer: 1778.7679443359375, output layer: 21.534971237182617\n",
      "\n",
      "Epoch 3, Test Acc: 73.66%, eta: 05:38:07 \n",
      "Averge spike counts: input layer: 3551.8896484375, second layer: 8019.64453125, third layer: 2187.580078125, output layer: 25.782365798950195\n",
      "\n",
      "Epoch 4, Test Acc: 74.14%, eta: 05:34:37 \n",
      "Averge spike counts: input layer: 3822.6923828125, second layer: 9011.2587890625, third layer: 2444.0595703125, output layer: 28.801340103149414\n",
      "\n",
      "Epoch 5, Test Acc: 76.56%, eta: 05:30:55 \n",
      "Averge spike counts: input layer: 3975.094970703125, second layer: 9595.396484375, third layer: 2511.3955078125, output layer: 28.805431365966797\n",
      "\n",
      "Epoch 6, Test Acc: 79.24%, eta: 05:27:22 \n",
      "Averge spike counts: input layer: 4019.7958984375, second layer: 9782.0478515625, third layer: 2566.121337890625, output layer: 30.670387268066406\n",
      "\n",
      "Epoch 7, Test Acc: 77.46%, eta: 05:23:55 \n",
      "Averge spike counts: input layer: 4099.455078125, second layer: 9978.0224609375, third layer: 2588.70849609375, output layer: 31.207218170166016\n",
      "\n",
      "Epoch 8, Test Acc: 80.02%, eta: 05:20:26 \n",
      "Averge spike counts: input layer: 4259.0185546875, second layer: 10409.0009765625, third layer: 2653.089599609375, output layer: 33.9226188659668\n",
      "\n",
      "Epoch 9, Test Acc: 80.69%, eta: 05:17:08 \n",
      "Averge spike counts: input layer: 4278.58447265625, second layer: 10476.802734375, third layer: 2606.5185546875, output layer: 32.806549072265625\n",
      "\n",
      "Epoch 10, Test Acc: 82.29%, eta: 05:13:35 \n",
      "Averge spike counts: input layer: 4326.39892578125, second layer: 10578.2587890625, third layer: 2666.640625, output layer: 34.74107360839844\n",
      "\n",
      "Epoch 11, Test Acc: 81.92%, eta: 05:10:10 \n",
      "Averge spike counts: input layer: 4396.74951171875, second layer: 10719.2431640625, third layer: 2702.984375, output layer: 36.328125\n",
      "\n",
      "Epoch 12, Test Acc: 81.51%, eta: 05:06:48 \n",
      "Averge spike counts: input layer: 4433.189453125, second layer: 10800.392578125, third layer: 2712.30517578125, output layer: 37.39955520629883\n",
      "\n",
      "Epoch 13, Test Acc: 83.04%, eta: 05:03:12 \n",
      "Averge spike counts: input layer: 4486.01416015625, second layer: 10859.7490234375, third layer: 2654.93408203125, output layer: 34.96577453613281\n",
      "\n",
      "Epoch 14, Test Acc: 82.55%, eta: 04:59:44 \n",
      "Averge spike counts: input layer: 4484.1259765625, second layer: 10783.9111328125, third layer: 2704.936767578125, output layer: 37.00669860839844\n",
      "\n",
      "Epoch 15, Test Acc: 82.18%, eta: 04:56:20 \n",
      "Averge spike counts: input layer: 4519.2607421875, second layer: 10765.4072265625, third layer: 2727.9287109375, output layer: 39.218379974365234\n",
      "\n",
      "Epoch 16, Test Acc: 82.03%, eta: 04:52:49 \n",
      "Averge spike counts: input layer: 4497.15185546875, second layer: 10640.8955078125, third layer: 2672.16064453125, output layer: 36.782737731933594\n",
      "\n",
      "Epoch 17, Test Acc: 83.33%, eta: 04:49:20 \n",
      "Averge spike counts: input layer: 4535.4306640625, second layer: 10604.9169921875, third layer: 2656.896728515625, output layer: 36.83035659790039\n",
      "\n",
      "Epoch 18, Test Acc: 83.07%, eta: 04:45:52 \n",
      "Averge spike counts: input layer: 4531.72900390625, second layer: 10522.79296875, third layer: 2642.385498046875, output layer: 37.6945686340332\n",
      "\n",
      "Epoch 19, Test Acc: 83.11%, eta: 04:42:22 \n",
      "Averge spike counts: input layer: 4517.2822265625, second layer: 10444.720703125, third layer: 2625.91552734375, output layer: 38.28236770629883\n",
      "\n",
      "Epoch 20, Test Acc: 81.70%, eta: 04:38:53 \n",
      "Averge spike counts: input layer: 4598.65234375, second layer: 10560.568359375, third layer: 2657.291015625, output layer: 39.61272430419922\n",
      "\n",
      "Epoch 21, Test Acc: 82.44%, eta: 04:35:40 \n",
      "Averge spike counts: input layer: 4557.583984375, second layer: 10369.4638671875, third layer: 2654.65966796875, output layer: 40.376487731933594\n",
      "\n",
      "Epoch 22, Test Acc: 83.67%, eta: 04:32:08 \n",
      "Averge spike counts: input layer: 4570.99951171875, second layer: 10295.599609375, third layer: 2643.39892578125, output layer: 40.8820686340332\n",
      "\n",
      "Epoch 23, Test Acc: 82.48%, eta: 04:28:39 \n",
      "Averge spike counts: input layer: 4619.96630859375, second layer: 10384.0087890625, third layer: 2641.580322265625, output layer: 41.236236572265625\n",
      "\n",
      "Epoch 24, Test Acc: 83.22%, eta: 04:25:07 \n",
      "Averge spike counts: input layer: 4633.22216796875, second layer: 10303.49609375, third layer: 2644.991943359375, output layer: 41.08891296386719\n",
      "\n",
      "Epoch 25, Test Acc: 85.12%, eta: 04:21:35 \n",
      "Averge spike counts: input layer: 4583.35302734375, second layer: 10161.3369140625, third layer: 2609.825927734375, output layer: 40.39285659790039\n",
      "\n",
      "Epoch 26, Test Acc: 84.30%, eta: 04:18:04 \n",
      "Averge spike counts: input layer: 4609.59814453125, second layer: 10118.4306640625, third layer: 2615.088623046875, output layer: 41.80952453613281\n",
      "\n",
      "Epoch 27, Test Acc: 82.25%, eta: 04:14:33 \n",
      "Averge spike counts: input layer: 4551.76953125, second layer: 9934.17578125, third layer: 2568.8076171875, output layer: 41.859004974365234\n",
      "\n",
      "Epoch 28, Test Acc: 84.15%, eta: 04:11:05 \n",
      "Averge spike counts: input layer: 4505.7265625, second layer: 9717.556640625, third layer: 2538.157470703125, output layer: 40.452754974365234\n",
      "\n",
      "Epoch 29, Test Acc: 83.67%, eta: 04:07:37 \n",
      "Averge spike counts: input layer: 4554.66455078125, second layer: 9756.2470703125, third layer: 2529.1044921875, output layer: 40.08891296386719\n",
      "\n",
      "Epoch 30, Test Acc: 84.11%, eta: 04:04:06 \n",
      "Averge spike counts: input layer: 4515.65283203125, second layer: 9639.966796875, third layer: 2527.521240234375, output layer: 39.38951110839844\n",
      "\n",
      "Epoch 31, Test Acc: 84.71%, eta: 04:00:37 \n",
      "Averge spike counts: input layer: 4546.36767578125, second layer: 9601.1083984375, third layer: 2536.811767578125, output layer: 40.05766296386719\n",
      "\n",
      "Epoch 32, Test Acc: 85.16%, eta: 03:57:26 \n",
      "Averge spike counts: input layer: 4604.08935546875, second layer: 9707.8603515625, third layer: 2520.592041015625, output layer: 40.1015625\n",
      "\n",
      "Epoch 33, Test Acc: 83.85%, eta: 03:53:55 \n",
      "Averge spike counts: input layer: 4574.5048828125, second layer: 9554.7294921875, third layer: 2514.2861328125, output layer: 39.97693634033203\n",
      "\n",
      "Epoch 34, Test Acc: 83.97%, eta: 03:50:23 \n",
      "Averge spike counts: input layer: 4536.3037109375, second layer: 9441.7353515625, third layer: 2510.697265625, output layer: 40.013023376464844\n",
      "\n",
      "Epoch 35, Test Acc: 85.08%, eta: 03:46:54 \n",
      "Averge spike counts: input layer: 4540.54345703125, second layer: 9374.669921875, third layer: 2505.59228515625, output layer: 40.31361770629883\n",
      "\n",
      "Epoch 36, Test Acc: 84.19%, eta: 03:43:23 \n",
      "Averge spike counts: input layer: 4549.31201171875, second layer: 9349.583984375, third layer: 2461.369873046875, output layer: 39.582218170166016\n",
      "\n",
      "Epoch 37, Test Acc: 84.26%, eta: 03:39:53 \n",
      "Averge spike counts: input layer: 4555.8623046875, second layer: 9243.6962890625, third layer: 2441.610107421875, output layer: 38.96354293823242\n",
      "\n",
      "Epoch 38, Test Acc: 85.08%, eta: 03:36:24 \n",
      "Averge spike counts: input layer: 4543.2001953125, second layer: 9156.4775390625, third layer: 2449.154541015625, output layer: 39.12611770629883\n",
      "\n",
      "Epoch 39, Test Acc: 84.60%, eta: 03:32:53 \n",
      "Averge spike counts: input layer: 4556.6591796875, second layer: 9164.8955078125, third layer: 2454.548828125, output layer: 39.578125\n",
      "\n",
      "Epoch 40, Test Acc: 84.41%, eta: 03:29:22 \n",
      "Averge spike counts: input layer: 4500.0185546875, second layer: 9020.51953125, third layer: 2429.3984375, output layer: 40.148067474365234\n",
      "\n",
      "Epoch 41, Test Acc: 84.30%, eta: 03:25:54 \n",
      "Averge spike counts: input layer: 4471.1552734375, second layer: 8874.0146484375, third layer: 2411.8974609375, output layer: 39.042781829833984\n",
      "\n",
      "Epoch 42, Test Acc: 85.45%, eta: 03:22:25 \n",
      "Averge spike counts: input layer: 4475.8232421875, second layer: 8848.9306640625, third layer: 2385.9404296875, output layer: 39.5944938659668\n",
      "\n",
      "Epoch 43, Test Acc: 85.42%, eta: 03:18:54 \n",
      "Averge spike counts: input layer: 4484.3603515625, second layer: 8761.3271484375, third layer: 2409.714599609375, output layer: 40.14732360839844\n",
      "\n",
      "Epoch 44, Test Acc: 83.48%, eta: 03:15:24 \n",
      "Averge spike counts: input layer: 4518.42138671875, second layer: 8834.681640625, third layer: 2417.519775390625, output layer: 40.75260543823242\n",
      "\n",
      "Epoch 45, Test Acc: 83.26%, eta: 03:11:53 \n",
      "Averge spike counts: input layer: 4488.48388671875, second layer: 8719.2021484375, third layer: 2379.829345703125, output layer: 40.050594329833984\n",
      "\n",
      "Epoch 46, Test Acc: 85.38%, eta: 03:08:23 \n",
      "Averge spike counts: input layer: 4501.728515625, second layer: 8687.572265625, third layer: 2373.37646484375, output layer: 39.826637268066406\n",
      "\n",
      "Epoch 47, Test Acc: 84.26%, eta: 03:04:54 \n",
      "Averge spike counts: input layer: 4353.935546875, second layer: 8405.5791015625, third layer: 2356.491943359375, output layer: 39.96763610839844\n",
      "\n",
      "Epoch 48, Test Acc: 84.19%, eta: 03:01:25 \n",
      "Averge spike counts: input layer: 4388.39013671875, second layer: 8409.68359375, third layer: 2353.58740234375, output layer: 39.199405670166016\n",
      "\n",
      "Epoch 49, Test Acc: 85.01%, eta: 02:57:55 \n",
      "Averge spike counts: input layer: 4407.91455078125, second layer: 8389.7490234375, third layer: 2334.69189453125, output layer: 38.59672546386719\n",
      "\n",
      "Epoch 50, Test Acc: 85.68%, eta: 02:54:27 \n",
      "Averge spike counts: input layer: 4363.55712890625, second layer: 8259.951171875, third layer: 2338.690185546875, output layer: 39.24107360839844\n",
      "\n",
      "Epoch 51, Test Acc: 85.16%, eta: 02:50:58 \n",
      "Averge spike counts: input layer: 4338.03076171875, second layer: 8251.798828125, third layer: 2297.994384765625, output layer: 38.70610046386719\n",
      "\n",
      "Epoch 52, Test Acc: 84.86%, eta: 02:47:28 \n",
      "Averge spike counts: input layer: 4409.4853515625, second layer: 8320.2783203125, third layer: 2336.53564453125, output layer: 39.49702453613281\n",
      "\n",
      "Epoch 53, Test Acc: 85.08%, eta: 02:43:58 \n",
      "Averge spike counts: input layer: 4367.40087890625, second layer: 8214.25390625, third layer: 2331.4345703125, output layer: 39.559898376464844\n",
      "\n",
      "Epoch 54, Test Acc: 85.19%, eta: 02:40:29 \n",
      "Averge spike counts: input layer: 4350.9990234375, second layer: 8208.3291015625, third layer: 2318.88330078125, output layer: 39.38169860839844\n",
      "\n",
      "Epoch 55, Test Acc: 85.38%, eta: 02:37:00 \n",
      "Averge spike counts: input layer: 4369.84619140625, second layer: 8118.34228515625, third layer: 2287.21923828125, output layer: 39.09858703613281\n",
      "\n",
      "Epoch 56, Test Acc: 85.04%, eta: 02:33:30 \n",
      "Averge spike counts: input layer: 4278.4541015625, second layer: 7965.96826171875, third layer: 2263.04541015625, output layer: 38.94233703613281\n",
      "\n",
      "Epoch 57, Test Acc: 84.82%, eta: 02:30:00 \n",
      "Averge spike counts: input layer: 4310.779296875, second layer: 8033.90283203125, third layer: 2275.8154296875, output layer: 39.51599884033203\n",
      "\n",
      "Epoch 58, Test Acc: 84.30%, eta: 02:26:31 \n",
      "Averge spike counts: input layer: 4242.4892578125, second layer: 7852.1953125, third layer: 2230.28955078125, output layer: 38.04018020629883\n",
      "\n",
      "Epoch 59, Test Acc: 86.27%, eta: 02:23:01 \n",
      "Averge spike counts: input layer: 4267.4248046875, second layer: 7860.67431640625, third layer: 2239.527587890625, output layer: 38.189361572265625\n",
      "\n",
      "Epoch 60, Test Acc: 85.31%, eta: 02:19:31 \n",
      "Averge spike counts: input layer: 4274.16845703125, second layer: 7806.12939453125, third layer: 2253.0732421875, output layer: 38.230281829833984\n",
      "\n",
      "Epoch 61, Test Acc: 85.42%, eta: 02:16:02 \n",
      "Averge spike counts: input layer: 4302.4384765625, second layer: 7871.4951171875, third layer: 2256.300537109375, output layer: 38.238094329833984\n",
      "\n",
      "Epoch 62, Test Acc: 85.42%, eta: 02:12:32 \n",
      "Averge spike counts: input layer: 4268.77685546875, second layer: 7761.6123046875, third layer: 2227.281982421875, output layer: 37.79352569580078\n",
      "\n",
      "Epoch 63, Test Acc: 86.20%, eta: 02:09:03 \n",
      "Averge spike counts: input layer: 4204.38037109375, second layer: 7595.18115234375, third layer: 2210.164794921875, output layer: 37.028648376464844\n",
      "\n",
      "Epoch 64, Test Acc: 85.42%, eta: 02:05:34 \n",
      "Averge spike counts: input layer: 4212.2451171875, second layer: 7637.15625, third layer: 2212.9091796875, output layer: 37.583335876464844\n",
      "\n",
      "Epoch 65, Test Acc: 86.05%, eta: 02:02:04 \n",
      "Averge spike counts: input layer: 4224.76806640625, second layer: 7673.63037109375, third layer: 2200.562255859375, output layer: 37.38764953613281\n",
      "\n",
      "Epoch 66, Test Acc: 84.60%, eta: 01:58:34 \n",
      "Averge spike counts: input layer: 4214.6474609375, second layer: 7684.95263671875, third layer: 2246.08349609375, output layer: 38.566219329833984\n",
      "\n",
      "Epoch 67, Test Acc: 86.61%, eta: 01:55:05 \n",
      "Averge spike counts: input layer: 4274.96484375, second layer: 7724.66845703125, third layer: 2246.0693359375, output layer: 38.18973159790039\n",
      "\n",
      "Epoch 68, Test Acc: 85.57%, eta: 01:51:36 \n",
      "Averge spike counts: input layer: 4165.5283203125, second layer: 7549.25439453125, third layer: 2199.570068359375, output layer: 37.317710876464844\n",
      "\n",
      "Epoch 69, Test Acc: 84.78%, eta: 01:48:06 \n",
      "Averge spike counts: input layer: 4205.759765625, second layer: 7659.77392578125, third layer: 2210.022705078125, output layer: 37.5866813659668\n",
      "\n",
      "Epoch 70, Test Acc: 85.45%, eta: 01:44:37 \n",
      "Averge spike counts: input layer: 4197.12255859375, second layer: 7636.70751953125, third layer: 2218.144287109375, output layer: 37.97916793823242\n",
      "\n",
      "Epoch 71, Test Acc: 85.38%, eta: 01:41:08 \n",
      "Averge spike counts: input layer: 4133.8056640625, second layer: 7406.3203125, third layer: 2247.71435546875, output layer: 37.99702453613281\n",
      "\n",
      "Epoch 72, Test Acc: 83.30%, eta: 01:37:40 \n",
      "Averge spike counts: input layer: 4099.474609375, second layer: 7393.87353515625, third layer: 2222.6279296875, output layer: 37.75\n",
      "\n",
      "Epoch 73, Test Acc: 85.86%, eta: 01:34:11 \n",
      "Averge spike counts: input layer: 4129.150390625, second layer: 7403.2294921875, third layer: 2199.636962890625, output layer: 36.7421875\n",
      "\n",
      "Epoch 74, Test Acc: 85.97%, eta: 01:30:41 \n",
      "Averge spike counts: input layer: 4099.45556640625, second layer: 7332.708984375, third layer: 2187.217041015625, output layer: 37.497398376464844\n",
      "\n",
      "Epoch 75, Test Acc: 85.12%, eta: 01:27:12 \n",
      "Averge spike counts: input layer: 4148.458984375, second layer: 7336.0654296875, third layer: 2216.361572265625, output layer: 37.0944938659668\n",
      "\n",
      "Epoch 76, Test Acc: 84.11%, eta: 01:23:42 \n",
      "Averge spike counts: input layer: 4056.49609375, second layer: 7218.6474609375, third layer: 2173.938720703125, output layer: 36.1804313659668\n",
      "\n",
      "Epoch 77, Test Acc: 86.53%, eta: 01:20:13 \n",
      "Averge spike counts: input layer: 4083.628173828125, second layer: 7274.880859375, third layer: 2207.26123046875, output layer: 37.1882438659668\n",
      "\n",
      "Epoch 78, Test Acc: 85.04%, eta: 01:16:44 \n",
      "Averge spike counts: input layer: 4065.778076171875, second layer: 7197.8720703125, third layer: 2188.61572265625, output layer: 36.808780670166016\n",
      "\n",
      "Epoch 79, Test Acc: 86.20%, eta: 01:13:14 \n",
      "Averge spike counts: input layer: 4038.592041015625, second layer: 7163.1689453125, third layer: 2186.710205078125, output layer: 36.973960876464844\n",
      "\n",
      "Epoch 80, Test Acc: 86.05%, eta: 01:09:48 \n",
      "Averge spike counts: input layer: 4088.0537109375, second layer: 7301.4853515625, third layer: 2174.3330078125, output layer: 36.497398376464844\n",
      "\n",
      "Epoch 81, Test Acc: 85.57%, eta: 01:06:19 \n",
      "Averge spike counts: input layer: 4046.8564453125, second layer: 7195.921875, third layer: 2192.921630859375, output layer: 37.27232360839844\n",
      "\n",
      "Epoch 82, Test Acc: 85.38%, eta: 01:02:49 \n",
      "Averge spike counts: input layer: 4009.288330078125, second layer: 7159.0341796875, third layer: 2161.015380859375, output layer: 36.603050231933594\n",
      "\n",
      "Epoch 83, Test Acc: 85.34%, eta: 00:59:20 \n",
      "Averge spike counts: input layer: 4049.022705078125, second layer: 7175.01513671875, third layer: 2174.13623046875, output layer: 36.20907974243164\n",
      "\n",
      "Epoch 84, Test Acc: 86.12%, eta: 00:55:50 \n",
      "Averge spike counts: input layer: 3975.439453125, second layer: 7006.671875, third layer: 2154.187255859375, output layer: 35.64657974243164\n",
      "\n",
      "Epoch 85, Test Acc: 85.45%, eta: 00:52:21 \n",
      "Averge spike counts: input layer: 3979.276123046875, second layer: 7053.6328125, third layer: 2145.419189453125, output layer: 35.639137268066406\n",
      "\n",
      "Epoch 86, Test Acc: 85.79%, eta: 00:48:51 \n",
      "Averge spike counts: input layer: 3943.47705078125, second layer: 6991.83740234375, third layer: 2145.099853515625, output layer: 35.663692474365234\n",
      "\n",
      "Epoch 87, Test Acc: 85.12%, eta: 00:45:22 \n",
      "Averge spike counts: input layer: 4045.9951171875, second layer: 7170.90478515625, third layer: 2168.047607421875, output layer: 35.622398376464844\n",
      "\n",
      "Epoch 88, Test Acc: 87.05%, eta: 00:41:53 \n",
      "Averge spike counts: input layer: 3945.002197265625, second layer: 7049.62939453125, third layer: 2183.2021484375, output layer: 36.874629974365234\n",
      "\n",
      "Epoch 89, Test Acc: 87.13%, eta: 00:38:23 \n",
      "Averge spike counts: input layer: 3979.6005859375, second layer: 7023.69873046875, third layer: 2161.045166015625, output layer: 36.329612731933594\n",
      "\n",
      "Epoch 90, Test Acc: 86.16%, eta: 00:34:55 \n",
      "Averge spike counts: input layer: 3956.97705078125, second layer: 7026.81103515625, third layer: 2182.08251953125, output layer: 37.03608703613281\n",
      "\n",
      "Epoch 91, Test Acc: 85.83%, eta: 00:31:26 \n",
      "Averge spike counts: input layer: 3866.657470703125, second layer: 6868.244140625, third layer: 2166.813232421875, output layer: 36.36198043823242\n",
      "\n",
      "Epoch 92, Test Acc: 85.90%, eta: 00:27:56 \n",
      "Averge spike counts: input layer: 3886.77880859375, second layer: 6894.5791015625, third layer: 2158.38916015625, output layer: 36.42745590209961\n",
      "\n",
      "Epoch 93, Test Acc: 85.97%, eta: 00:24:26 \n",
      "Averge spike counts: input layer: 3895.4052734375, second layer: 6957.9658203125, third layer: 2187.296630859375, output layer: 37.121280670166016\n",
      "\n",
      "Epoch 94, Test Acc: 85.60%, eta: 00:20:57 \n",
      "Averge spike counts: input layer: 3904.390380859375, second layer: 6947.16845703125, third layer: 2171.7412109375, output layer: 37.119049072265625\n",
      "\n",
      "Epoch 95, Test Acc: 86.76%, eta: 00:17:27 \n",
      "Averge spike counts: input layer: 3857.1953125, second layer: 6868.6533203125, third layer: 2165.276123046875, output layer: 37.0085563659668\n",
      "\n",
      "Epoch 96, Test Acc: 85.19%, eta: 00:13:58 \n",
      "Averge spike counts: input layer: 3876.5830078125, second layer: 6891.9072265625, third layer: 2183.46533203125, output layer: 36.93080520629883\n",
      "\n",
      "Epoch 97, Test Acc: 85.53%, eta: 00:10:28 \n",
      "Averge spike counts: input layer: 3869.607177734375, second layer: 6819.96142578125, third layer: 2155.006103515625, output layer: 36.587799072265625\n",
      "\n",
      "Epoch 98, Test Acc: 86.76%, eta: 00:06:59 \n",
      "Averge spike counts: input layer: 3823.510009765625, second layer: 6752.70849609375, third layer: 2155.251953125, output layer: 36.80245590209961\n",
      "\n",
      "Epoch 99, Test Acc: 84.90%, eta: 00:03:29 \n",
      "Averge spike counts: input layer: 3799.40380859375, second layer: 6669.7841796875, third layer: 2121.15185546875, output layer: 36.103050231933594\n",
      "\n",
      "Epoch 100, Test Acc: 85.79%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 3750.888427734375, second layer: 6574.21826171875, third layer: 2120.239501953125, output layer: 36.12760543823242\n",
      "\n",
      "20953.087543964386 seconds\n",
      "\n",
      "medium initial beta experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_med_beta = Net_medium_initial_beta().to(device)\n",
    "net_med_beta = train(net_med_beta)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"medium initial beta experiment\\n\")\n",
    "torch.save(net_med_beta.state_dict(), 'model_med_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 84.56%, eta: 05:46:20 \n",
      "Averge spike counts: input layer: 6988.43701171875, second layer: 33018.45703125, third layer: 20005.404296875, output layer: 44.662574768066406\n",
      "\n",
      "Epoch 2, Test Acc: 85.01%, eta: 05:42:17 \n",
      "Averge spike counts: input layer: 6937.341796875, second layer: 32927.06640625, third layer: 20006.47265625, output layer: 44.718379974365234\n",
      "\n",
      "Epoch 3, Test Acc: 85.53%, eta: 05:38:28 \n",
      "Averge spike counts: input layer: 6947.0791015625, second layer: 32942.46875, third layer: 19993.1796875, output layer: 45.01674270629883\n",
      "\n",
      "Epoch 4, Test Acc: 83.89%, eta: 05:35:01 \n",
      "Averge spike counts: input layer: 7027.20849609375, second layer: 33089.48828125, third layer: 19999.109375, output layer: 44.253719329833984\n",
      "\n",
      "Epoch 5, Test Acc: 85.57%, eta: 05:31:28 \n",
      "Averge spike counts: input layer: 6958.0537109375, second layer: 32973.515625, third layer: 20002.275390625, output layer: 44.427085876464844\n",
      "\n",
      "Epoch 6, Test Acc: 83.82%, eta: 05:28:10 \n",
      "Averge spike counts: input layer: 7000.07763671875, second layer: 32998.70703125, third layer: 19996.830078125, output layer: 44.620906829833984\n",
      "\n",
      "Epoch 7, Test Acc: 85.79%, eta: 05:24:48 \n",
      "Averge spike counts: input layer: 7038.69189453125, second layer: 33056.7265625, third layer: 20002.22265625, output layer: 46.85974884033203\n",
      "\n",
      "Epoch 8, Test Acc: 84.86%, eta: 05:21:10 \n",
      "Averge spike counts: input layer: 6994.89013671875, second layer: 32974.125, third layer: 20002.021484375, output layer: 44.78943634033203\n",
      "\n",
      "Epoch 9, Test Acc: 86.16%, eta: 05:17:34 \n",
      "Averge spike counts: input layer: 7018.33056640625, second layer: 33011.66015625, third layer: 20002.591796875, output layer: 44.37946701049805\n",
      "\n",
      "Epoch 10, Test Acc: 84.49%, eta: 05:14:01 \n",
      "Averge spike counts: input layer: 7030.18603515625, second layer: 33035.015625, third layer: 20008.4765625, output layer: 47.592262268066406\n",
      "\n",
      "Epoch 11, Test Acc: 84.49%, eta: 05:10:32 \n",
      "Averge spike counts: input layer: 7009.5703125, second layer: 32964.82421875, third layer: 19995.79296875, output layer: 48.488094329833984\n",
      "\n",
      "Epoch 12, Test Acc: 83.82%, eta: 05:06:57 \n",
      "Averge spike counts: input layer: 7039.318359375, second layer: 33016.6015625, third layer: 20008.669921875, output layer: 47.27157974243164\n",
      "\n",
      "Epoch 13, Test Acc: 84.97%, eta: 05:03:23 \n",
      "Averge spike counts: input layer: 7011.08935546875, second layer: 32979.390625, third layer: 20007.330078125, output layer: 46.808780670166016\n",
      "\n",
      "Epoch 14, Test Acc: 84.19%, eta: 04:59:51 \n",
      "Averge spike counts: input layer: 7055.11474609375, second layer: 33031.8984375, third layer: 20005.03515625, output layer: 47.05022430419922\n",
      "\n",
      "Epoch 15, Test Acc: 84.60%, eta: 04:56:29 \n",
      "Averge spike counts: input layer: 7093.3662109375, second layer: 33088.05859375, third layer: 19999.96484375, output layer: 45.613094329833984\n",
      "\n",
      "Epoch 16, Test Acc: 84.56%, eta: 04:53:08 \n",
      "Averge spike counts: input layer: 7058.02978515625, second layer: 33033.37890625, third layer: 20001.32421875, output layer: 45.722843170166016\n",
      "\n",
      "Epoch 17, Test Acc: 84.71%, eta: 04:49:35 \n",
      "Averge spike counts: input layer: 6982.18310546875, second layer: 32935.9140625, third layer: 20001.0546875, output layer: 43.113094329833984\n",
      "\n",
      "Epoch 18, Test Acc: 85.49%, eta: 04:46:06 \n",
      "Averge spike counts: input layer: 6943.6787109375, second layer: 32866.80859375, third layer: 20002.08203125, output layer: 43.1882438659668\n",
      "\n",
      "Epoch 19, Test Acc: 84.60%, eta: 04:42:36 \n",
      "Averge spike counts: input layer: 7011.93310546875, second layer: 32980.66796875, third layer: 20004.09765625, output layer: 43.83258819580078\n",
      "\n",
      "Epoch 20, Test Acc: 85.34%, eta: 04:39:06 \n",
      "Averge spike counts: input layer: 7017.85302734375, second layer: 32942.8828125, third layer: 20000.87890625, output layer: 43.46131134033203\n",
      "\n",
      "Epoch 21, Test Acc: 84.67%, eta: 04:35:34 \n",
      "Averge spike counts: input layer: 6992.72802734375, second layer: 32922.80078125, third layer: 20004.53515625, output layer: 45.4069938659668\n",
      "\n",
      "Epoch 22, Test Acc: 85.71%, eta: 04:32:05 \n",
      "Averge spike counts: input layer: 7013.087890625, second layer: 32966.046875, third layer: 20004.8203125, output layer: 44.36941909790039\n",
      "\n",
      "Epoch 23, Test Acc: 85.08%, eta: 04:28:35 \n",
      "Averge spike counts: input layer: 6979.8125, second layer: 32902.7578125, third layer: 19992.763671875, output layer: 43.285343170166016\n",
      "\n",
      "Epoch 24, Test Acc: 84.60%, eta: 04:25:07 \n",
      "Averge spike counts: input layer: 7029.87939453125, second layer: 32970.46484375, third layer: 19996.99609375, output layer: 46.52678680419922\n",
      "\n",
      "Epoch 25, Test Acc: 84.60%, eta: 04:21:36 \n",
      "Averge spike counts: input layer: 7032.64990234375, second layer: 32969.34375, third layer: 19999.79296875, output layer: 43.784969329833984\n",
      "\n",
      "Epoch 26, Test Acc: 84.93%, eta: 04:18:06 \n",
      "Averge spike counts: input layer: 7059.0732421875, second layer: 33017.8203125, third layer: 20001.607421875, output layer: 43.58705520629883\n",
      "\n",
      "Epoch 27, Test Acc: 85.64%, eta: 04:14:35 \n",
      "Averge spike counts: input layer: 7024.35791015625, second layer: 32968.53515625, third layer: 20003.88671875, output layer: 44.27566909790039\n",
      "\n",
      "Epoch 28, Test Acc: 85.97%, eta: 04:11:08 \n",
      "Averge spike counts: input layer: 6901.48828125, second layer: 32798.0078125, third layer: 20004.41796875, output layer: 42.68861770629883\n",
      "\n",
      "Epoch 29, Test Acc: 85.12%, eta: 04:07:37 \n",
      "Averge spike counts: input layer: 7051.3876953125, second layer: 32961.93359375, third layer: 20002.810546875, output layer: 45.2351188659668\n",
      "\n",
      "Epoch 30, Test Acc: 85.27%, eta: 04:04:06 \n",
      "Averge spike counts: input layer: 6976.38720703125, second layer: 32890.2578125, third layer: 20003.80078125, output layer: 43.1328125\n",
      "\n",
      "Epoch 31, Test Acc: 84.49%, eta: 04:00:36 \n",
      "Averge spike counts: input layer: 6984.2607421875, second layer: 32883.546875, third layer: 20002.822265625, output layer: 44.434898376464844\n",
      "\n",
      "Epoch 32, Test Acc: 84.75%, eta: 03:57:06 \n",
      "Averge spike counts: input layer: 6962.6357421875, second layer: 32821.5546875, third layer: 19995.7109375, output layer: 46.21316909790039\n",
      "\n",
      "Epoch 33, Test Acc: 85.94%, eta: 03:53:39 \n",
      "Averge spike counts: input layer: 6989.77392578125, second layer: 32872.5234375, third layer: 19999.10546875, output layer: 45.908111572265625\n",
      "\n",
      "Epoch 34, Test Acc: 86.16%, eta: 03:50:09 \n",
      "Averge spike counts: input layer: 6971.7109375, second layer: 32801.99609375, third layer: 19986.6796875, output layer: 43.688987731933594\n",
      "\n",
      "Epoch 35, Test Acc: 84.45%, eta: 03:46:38 \n",
      "Averge spike counts: input layer: 6993.12939453125, second layer: 32836.6953125, third layer: 19990.685546875, output layer: 44.38727951049805\n",
      "\n",
      "Epoch 36, Test Acc: 85.86%, eta: 03:43:09 \n",
      "Averge spike counts: input layer: 6984.19970703125, second layer: 32841.66796875, third layer: 19982.6796875, output layer: 44.99665451049805\n",
      "\n",
      "Epoch 37, Test Acc: 84.86%, eta: 03:39:41 \n",
      "Averge spike counts: input layer: 7014.8291015625, second layer: 32892.375, third layer: 19994.306640625, output layer: 44.12760543823242\n",
      "\n",
      "Epoch 38, Test Acc: 85.42%, eta: 03:36:12 \n",
      "Averge spike counts: input layer: 7064.419921875, second layer: 32917.70703125, third layer: 19997.21875, output layer: 46.16220474243164\n",
      "\n",
      "Epoch 39, Test Acc: 85.23%, eta: 03:32:43 \n",
      "Averge spike counts: input layer: 6980.421875, second layer: 32838.03515625, third layer: 20002.126953125, output layer: 43.872398376464844\n",
      "\n",
      "Epoch 40, Test Acc: 85.60%, eta: 03:29:14 \n",
      "Averge spike counts: input layer: 7060.63720703125, second layer: 32929.171875, third layer: 19994.55859375, output layer: 44.34858703613281\n",
      "\n",
      "Epoch 41, Test Acc: 84.71%, eta: 03:25:46 \n",
      "Averge spike counts: input layer: 6978.09228515625, second layer: 32802.17578125, third layer: 19987.92578125, output layer: 43.69754409790039\n",
      "\n",
      "Epoch 42, Test Acc: 85.31%, eta: 03:22:16 \n",
      "Averge spike counts: input layer: 7034.7294921875, second layer: 32851.78515625, third layer: 19991.6015625, output layer: 42.4694938659668\n",
      "\n",
      "Epoch 43, Test Acc: 84.93%, eta: 03:18:46 \n",
      "Averge spike counts: input layer: 7024.28125, second layer: 32872.875, third layer: 20000.087890625, output layer: 43.660343170166016\n",
      "\n",
      "Epoch 44, Test Acc: 85.12%, eta: 03:15:16 \n",
      "Averge spike counts: input layer: 7025.33056640625, second layer: 32842.8125, third layer: 19991.748046875, output layer: 43.25446319580078\n",
      "\n",
      "Epoch 45, Test Acc: 86.61%, eta: 03:11:46 \n",
      "Averge spike counts: input layer: 7016.86767578125, second layer: 32854.640625, third layer: 19989.9921875, output layer: 42.369049072265625\n",
      "\n",
      "Epoch 46, Test Acc: 85.23%, eta: 03:08:21 \n",
      "Averge spike counts: input layer: 7002.68896484375, second layer: 32845.45703125, third layer: 19998.435546875, output layer: 43.574405670166016\n",
      "\n",
      "Epoch 47, Test Acc: 86.12%, eta: 03:04:51 \n",
      "Averge spike counts: input layer: 6979.03271484375, second layer: 32826.62890625, third layer: 19993.205078125, output layer: 42.794273376464844\n",
      "\n",
      "Epoch 48, Test Acc: 85.94%, eta: 03:01:21 \n",
      "Averge spike counts: input layer: 6978.25439453125, second layer: 32825.0703125, third layer: 19995.126953125, output layer: 44.54724884033203\n",
      "\n",
      "Epoch 49, Test Acc: 86.05%, eta: 02:57:52 \n",
      "Averge spike counts: input layer: 6962.31103515625, second layer: 32772.7265625, third layer: 19999.169921875, output layer: 43.759300231933594\n",
      "\n",
      "Epoch 50, Test Acc: 84.26%, eta: 02:54:23 \n",
      "Averge spike counts: input layer: 6991.138671875, second layer: 32821.0078125, third layer: 19998.94921875, output layer: 44.29203796386719\n",
      "\n",
      "Epoch 51, Test Acc: 83.67%, eta: 02:50:53 \n",
      "Averge spike counts: input layer: 6982.150390625, second layer: 32787.8515625, third layer: 19995.3828125, output layer: 42.05506134033203\n",
      "\n",
      "Epoch 52, Test Acc: 84.78%, eta: 02:47:23 \n",
      "Averge spike counts: input layer: 7012.80810546875, second layer: 32804.6328125, third layer: 19989.453125, output layer: 42.89657974243164\n",
      "\n",
      "Epoch 53, Test Acc: 86.27%, eta: 02:43:53 \n",
      "Averge spike counts: input layer: 6981.5029296875, second layer: 32780.24609375, third layer: 20001.904296875, output layer: 41.4055061340332\n",
      "\n",
      "Epoch 54, Test Acc: 85.27%, eta: 02:40:24 \n",
      "Averge spike counts: input layer: 7038.44189453125, second layer: 32852.91015625, third layer: 19994.87109375, output layer: 42.5632438659668\n",
      "\n",
      "Epoch 55, Test Acc: 85.23%, eta: 02:36:56 \n",
      "Averge spike counts: input layer: 7035.0341796875, second layer: 32829.296875, third layer: 19991.361328125, output layer: 44.27046203613281\n",
      "\n",
      "Epoch 56, Test Acc: 84.75%, eta: 02:33:27 \n",
      "Averge spike counts: input layer: 7000.509765625, second layer: 32811.67578125, third layer: 19992.998046875, output layer: 44.8742561340332\n",
      "\n",
      "Epoch 57, Test Acc: 84.82%, eta: 02:29:59 \n",
      "Averge spike counts: input layer: 7013.337890625, second layer: 32817.71875, third layer: 19989.0234375, output layer: 44.5554313659668\n",
      "\n",
      "Epoch 58, Test Acc: 85.57%, eta: 02:26:30 \n",
      "Averge spike counts: input layer: 6995.45556640625, second layer: 32747.66796875, third layer: 19981.154296875, output layer: 43.618675231933594\n",
      "\n",
      "Epoch 59, Test Acc: 85.23%, eta: 02:23:01 \n",
      "Averge spike counts: input layer: 7057.74853515625, second layer: 32820.859375, third layer: 19984.1796875, output layer: 44.86607360839844\n",
      "\n",
      "Epoch 60, Test Acc: 84.34%, eta: 02:19:31 \n",
      "Averge spike counts: input layer: 6997.4169921875, second layer: 32763.2890625, third layer: 19980.626953125, output layer: 43.431175231933594\n",
      "\n",
      "Epoch 61, Test Acc: 85.34%, eta: 02:16:01 \n",
      "Averge spike counts: input layer: 7062.166015625, second layer: 32867.78515625, third layer: 19993.248046875, output layer: 43.564361572265625\n",
      "\n",
      "Epoch 62, Test Acc: 85.68%, eta: 02:12:32 \n",
      "Averge spike counts: input layer: 7033.13720703125, second layer: 32827.30859375, third layer: 19995.853515625, output layer: 43.223960876464844\n",
      "\n",
      "Epoch 63, Test Acc: 85.08%, eta: 02:09:03 \n",
      "Averge spike counts: input layer: 7056.09228515625, second layer: 32847.09765625, third layer: 20000.2109375, output layer: 43.50297546386719\n",
      "\n",
      "Epoch 64, Test Acc: 85.34%, eta: 02:05:33 \n",
      "Averge spike counts: input layer: 7037.9375, second layer: 32827.1875, third layer: 19994.185546875, output layer: 44.62946701049805\n",
      "\n",
      "Epoch 65, Test Acc: 85.31%, eta: 02:02:04 \n",
      "Averge spike counts: input layer: 7091.11181640625, second layer: 32876.484375, third layer: 19996.5859375, output layer: 42.31138610839844\n",
      "\n",
      "Epoch 66, Test Acc: 84.93%, eta: 01:58:35 \n",
      "Averge spike counts: input layer: 7044.3603515625, second layer: 32834.1328125, third layer: 20002.947265625, output layer: 42.130210876464844\n",
      "\n",
      "Epoch 67, Test Acc: 85.57%, eta: 01:55:06 \n",
      "Averge spike counts: input layer: 7034.357421875, second layer: 32816.8515625, third layer: 19997.048828125, output layer: 42.12202453613281\n",
      "\n",
      "Epoch 68, Test Acc: 85.34%, eta: 01:51:36 \n",
      "Averge spike counts: input layer: 7079.3125, second layer: 32877.421875, third layer: 19999.44140625, output layer: 41.093379974365234\n",
      "\n",
      "Epoch 69, Test Acc: 85.08%, eta: 01:48:06 \n",
      "Averge spike counts: input layer: 7124.73095703125, second layer: 32914.70703125, third layer: 19995.09375, output layer: 42.40066909790039\n",
      "\n",
      "Epoch 70, Test Acc: 85.57%, eta: 01:44:37 \n",
      "Averge spike counts: input layer: 7065.2666015625, second layer: 32813.88671875, third layer: 19991.31640625, output layer: 42.21577453613281\n",
      "\n",
      "Epoch 71, Test Acc: 85.94%, eta: 01:41:07 \n",
      "Averge spike counts: input layer: 7059.4912109375, second layer: 32790.671875, third layer: 20001.6875, output layer: 43.3288688659668\n",
      "\n",
      "Epoch 72, Test Acc: 85.71%, eta: 01:37:38 \n",
      "Averge spike counts: input layer: 7072.568359375, second layer: 32831.85546875, third layer: 19992.109375, output layer: 40.93526840209961\n",
      "\n",
      "Epoch 73, Test Acc: 85.04%, eta: 01:34:08 \n",
      "Averge spike counts: input layer: 7041.15576171875, second layer: 32792.171875, third layer: 20006.310546875, output layer: 45.23251724243164\n",
      "\n",
      "Epoch 74, Test Acc: 86.57%, eta: 01:30:39 \n",
      "Averge spike counts: input layer: 7058.87939453125, second layer: 32801.93359375, third layer: 19993.87890625, output layer: 43.871280670166016\n",
      "\n",
      "Epoch 75, Test Acc: 85.60%, eta: 01:27:10 \n",
      "Averge spike counts: input layer: 7030.92431640625, second layer: 32746.11328125, third layer: 19986.41796875, output layer: 40.621280670166016\n",
      "\n",
      "Epoch 76, Test Acc: 86.24%, eta: 01:23:41 \n",
      "Averge spike counts: input layer: 7021.873046875, second layer: 32731.005859375, third layer: 19992.185546875, output layer: 44.72879409790039\n",
      "\n",
      "Epoch 77, Test Acc: 87.24%, eta: 01:20:12 \n",
      "Averge spike counts: input layer: 7006.14990234375, second layer: 32723.50390625, third layer: 19995.5859375, output layer: 45.6101188659668\n",
      "\n",
      "Epoch 78, Test Acc: 84.49%, eta: 01:16:42 \n",
      "Averge spike counts: input layer: 7044.2265625, second layer: 32754.134765625, third layer: 19980.65234375, output layer: 40.467262268066406\n",
      "\n",
      "Epoch 79, Test Acc: 84.97%, eta: 01:13:13 \n",
      "Averge spike counts: input layer: 7071.50439453125, second layer: 32754.669921875, third layer: 19976.69921875, output layer: 39.962799072265625\n",
      "\n",
      "Epoch 80, Test Acc: 85.97%, eta: 01:09:44 \n",
      "Averge spike counts: input layer: 7055.556640625, second layer: 32750.16796875, third layer: 19988.17578125, output layer: 42.763023376464844\n",
      "\n",
      "Epoch 81, Test Acc: 85.97%, eta: 01:06:15 \n",
      "Averge spike counts: input layer: 7082.02392578125, second layer: 32763.560546875, third layer: 19983.421875, output layer: 43.839656829833984\n",
      "\n",
      "Epoch 82, Test Acc: 86.05%, eta: 01:02:45 \n",
      "Averge spike counts: input layer: 7043.79931640625, second layer: 32737.3125, third layer: 19986.001953125, output layer: 43.06287384033203\n",
      "\n",
      "Epoch 83, Test Acc: 85.04%, eta: 00:59:16 \n",
      "Averge spike counts: input layer: 7037.90625, second layer: 32711.107421875, third layer: 19989.2578125, output layer: 43.90625\n",
      "\n",
      "Epoch 84, Test Acc: 86.46%, eta: 00:55:47 \n",
      "Averge spike counts: input layer: 6991.28271484375, second layer: 32646.259765625, third layer: 19990.55859375, output layer: 42.3203125\n",
      "\n",
      "Epoch 85, Test Acc: 85.42%, eta: 00:52:18 \n",
      "Averge spike counts: input layer: 7019.09228515625, second layer: 32670.775390625, third layer: 19985.201171875, output layer: 41.451637268066406\n",
      "\n",
      "Epoch 86, Test Acc: 85.94%, eta: 00:48:48 \n",
      "Averge spike counts: input layer: 7053.37451171875, second layer: 32698.60546875, third layer: 19986.609375, output layer: 41.331844329833984\n",
      "\n",
      "Epoch 87, Test Acc: 86.01%, eta: 00:45:19 \n",
      "Averge spike counts: input layer: 7077.25439453125, second layer: 32758.408203125, third layer: 19991.615234375, output layer: 42.98326110839844\n",
      "\n",
      "Epoch 88, Test Acc: 85.08%, eta: 00:41:50 \n",
      "Averge spike counts: input layer: 6993.11328125, second layer: 32634.8125, third layer: 19983.705078125, output layer: 44.54352951049805\n",
      "\n",
      "Epoch 89, Test Acc: 84.52%, eta: 00:38:20 \n",
      "Averge spike counts: input layer: 7041.05078125, second layer: 32708.595703125, third layer: 19981.69140625, output layer: 43.277530670166016\n",
      "\n",
      "Epoch 90, Test Acc: 87.09%, eta: 00:34:51 \n",
      "Averge spike counts: input layer: 7082.3779296875, second layer: 32779.828125, third layer: 19983.66796875, output layer: 44.61495590209961\n",
      "\n",
      "Epoch 91, Test Acc: 85.01%, eta: 00:31:22 \n",
      "Averge spike counts: input layer: 7022.64453125, second layer: 32721.017578125, third layer: 19991.8046875, output layer: 45.4538688659668\n",
      "\n",
      "Epoch 92, Test Acc: 85.45%, eta: 00:27:53 \n",
      "Averge spike counts: input layer: 7073.1279296875, second layer: 32783.359375, third layer: 19990.50390625, output layer: 42.99479293823242\n",
      "\n",
      "Epoch 93, Test Acc: 85.45%, eta: 00:24:24 \n",
      "Averge spike counts: input layer: 7063.64453125, second layer: 32752.548828125, third layer: 19978.087890625, output layer: 43.875\n",
      "\n",
      "Epoch 94, Test Acc: 85.79%, eta: 00:20:55 \n",
      "Averge spike counts: input layer: 7090.14453125, second layer: 32791.796875, third layer: 19980.912109375, output layer: 42.59040069580078\n",
      "\n",
      "Epoch 95, Test Acc: 86.64%, eta: 00:17:25 \n",
      "Averge spike counts: input layer: 7038.46826171875, second layer: 32715.294921875, third layer: 19986.794921875, output layer: 41.87314224243164\n",
      "\n",
      "Epoch 96, Test Acc: 86.64%, eta: 00:13:56 \n",
      "Averge spike counts: input layer: 7031.484375, second layer: 32676.66796875, third layer: 19995.26953125, output layer: 42.07514953613281\n",
      "\n",
      "Epoch 97, Test Acc: 85.90%, eta: 00:10:27 \n",
      "Averge spike counts: input layer: 6981.66552734375, second layer: 32635.4921875, third layer: 20001.330078125, output layer: 42.546504974365234\n",
      "\n",
      "Epoch 98, Test Acc: 85.12%, eta: 00:06:58 \n",
      "Averge spike counts: input layer: 7017.14306640625, second layer: 32675.275390625, third layer: 19986.83203125, output layer: 44.14509201049805\n",
      "\n",
      "Epoch 99, Test Acc: 85.16%, eta: 00:03:29 \n",
      "Averge spike counts: input layer: 7045.24560546875, second layer: 32706.462890625, third layer: 19991.29296875, output layer: 42.37834930419922\n",
      "\n",
      "Epoch 100, Test Acc: 84.97%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 7084.16064453125, second layer: 32733.73828125, third layer: 19988.919921875, output layer: 43.65513610839844\n",
      "\n",
      "20919.98610019684 seconds\n",
      "\n",
      "higher initial beta experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_high_beta = Net_high_initial_beta().to(device)\n",
    "net_high_beta = train(net_high_weights)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"higher initial beta experiment\\n\")\n",
    "torch.save(net_high_beta.state_dict(), 'model_high_beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 54.54%, eta: 05:35:28 \n",
      "Averge spike counts: input layer: 2016.2545166015625, second layer: 2961.334228515625, third layer: 1048.9869384765625, output layer: 10.450521469116211\n",
      "\n",
      "Epoch 2, Test Acc: 71.84%, eta: 05:32:46 \n",
      "Averge spike counts: input layer: 2410.9287109375, second layer: 4542.97314453125, third layer: 1894.252685546875, output layer: 26.394718170166016\n",
      "\n",
      "Epoch 3, Test Acc: 73.07%, eta: 05:28:48 \n",
      "Averge spike counts: input layer: 2629.85693359375, second layer: 5317.12841796875, third layer: 2239.409912109375, output layer: 28.129465103149414\n",
      "\n",
      "Epoch 4, Test Acc: 77.75%, eta: 05:25:28 \n",
      "Averge spike counts: input layer: 2788.66552734375, second layer: 5770.87548828125, third layer: 2410.392822265625, output layer: 26.707590103149414\n",
      "\n",
      "Epoch 5, Test Acc: 78.20%, eta: 05:22:01 \n",
      "Averge spike counts: input layer: 2778.40966796875, second layer: 5813.3994140625, third layer: 2453.28173828125, output layer: 28.0859375\n",
      "\n",
      "Epoch 6, Test Acc: 79.72%, eta: 05:18:39 \n",
      "Averge spike counts: input layer: 2828.830322265625, second layer: 5930.5849609375, third layer: 2478.401123046875, output layer: 26.980655670166016\n",
      "\n",
      "Epoch 7, Test Acc: 78.31%, eta: 05:15:08 \n",
      "Averge spike counts: input layer: 2902.097900390625, second layer: 6062.41845703125, third layer: 2545.468505859375, output layer: 27.43303680419922\n",
      "\n",
      "Epoch 8, Test Acc: 81.44%, eta: 05:11:50 \n",
      "Averge spike counts: input layer: 2918.91943359375, second layer: 6075.10888671875, third layer: 2526.821044921875, output layer: 26.137649536132812\n",
      "\n",
      "Epoch 9, Test Acc: 79.06%, eta: 05:08:25 \n",
      "Averge spike counts: input layer: 2939.430419921875, second layer: 6051.71826171875, third layer: 2514.7109375, output layer: 26.873512268066406\n",
      "\n",
      "Epoch 10, Test Acc: 80.62%, eta: 05:05:17 \n",
      "Averge spike counts: input layer: 3003.298095703125, second layer: 6159.89306640625, third layer: 2552.266845703125, output layer: 27.957962036132812\n",
      "\n",
      "Epoch 11, Test Acc: 81.62%, eta: 05:02:05 \n",
      "Averge spike counts: input layer: 3013.947998046875, second layer: 6107.1904296875, third layer: 2536.939697265625, output layer: 27.053199768066406\n",
      "\n",
      "Epoch 12, Test Acc: 81.58%, eta: 04:58:36 \n",
      "Averge spike counts: input layer: 2987.302490234375, second layer: 5993.26220703125, third layer: 2490.490478515625, output layer: 26.986980438232422\n",
      "\n",
      "Epoch 13, Test Acc: 81.96%, eta: 04:55:11 \n",
      "Averge spike counts: input layer: 3006.643310546875, second layer: 5980.23388671875, third layer: 2476.9267578125, output layer: 26.180431365966797\n",
      "\n",
      "Epoch 14, Test Acc: 82.81%, eta: 04:51:50 \n",
      "Averge spike counts: input layer: 2983.4052734375, second layer: 5866.20654296875, third layer: 2448.71533203125, output layer: 27.084821701049805\n",
      "\n",
      "Epoch 15, Test Acc: 82.55%, eta: 04:48:28 \n",
      "Averge spike counts: input layer: 3002.736572265625, second layer: 5820.30078125, third layer: 2431.597412109375, output layer: 24.98772430419922\n",
      "\n",
      "Epoch 16, Test Acc: 83.37%, eta: 04:45:04 \n",
      "Averge spike counts: input layer: 3011.566650390625, second layer: 5762.92333984375, third layer: 2406.090087890625, output layer: 25.443824768066406\n",
      "\n",
      "Epoch 17, Test Acc: 81.32%, eta: 04:41:43 \n",
      "Averge spike counts: input layer: 2993.23974609375, second layer: 5684.6142578125, third layer: 2406.769775390625, output layer: 25.98214340209961\n",
      "\n",
      "Epoch 18, Test Acc: 83.18%, eta: 04:38:17 \n",
      "Averge spike counts: input layer: 2976.052978515625, second layer: 5575.69970703125, third layer: 2381.33447265625, output layer: 26.029762268066406\n",
      "\n",
      "Epoch 19, Test Acc: 82.74%, eta: 04:34:54 \n",
      "Averge spike counts: input layer: 2983.662353515625, second layer: 5529.9951171875, third layer: 2363.38134765625, output layer: 24.955730438232422\n",
      "\n",
      "Epoch 20, Test Acc: 82.89%, eta: 04:31:34 \n",
      "Averge spike counts: input layer: 2993.76708984375, second layer: 5492.4072265625, third layer: 2344.36279296875, output layer: 24.45610237121582\n",
      "\n",
      "Epoch 21, Test Acc: 81.96%, eta: 04:28:09 \n",
      "Averge spike counts: input layer: 3038.522705078125, second layer: 5529.14794921875, third layer: 2358.32080078125, output layer: 25.040550231933594\n",
      "\n",
      "Epoch 22, Test Acc: 83.78%, eta: 04:24:44 \n",
      "Averge spike counts: input layer: 3034.024658203125, second layer: 5489.0751953125, third layer: 2338.078857421875, output layer: 24.04427146911621\n",
      "\n",
      "Epoch 23, Test Acc: 83.44%, eta: 04:21:21 \n",
      "Averge spike counts: input layer: 3046.106201171875, second layer: 5466.04833984375, third layer: 2342.132568359375, output layer: 25.139137268066406\n",
      "\n",
      "Epoch 24, Test Acc: 85.68%, eta: 04:17:54 \n",
      "Averge spike counts: input layer: 3001.622802734375, second layer: 5339.15283203125, third layer: 2286.607666015625, output layer: 24.326637268066406\n",
      "\n",
      "Epoch 25, Test Acc: 84.00%, eta: 04:14:30 \n",
      "Averge spike counts: input layer: 2993.235107421875, second layer: 5279.96826171875, third layer: 2291.8564453125, output layer: 24.821800231933594\n",
      "\n",
      "Epoch 26, Test Acc: 84.45%, eta: 04:11:06 \n",
      "Averge spike counts: input layer: 2982.891845703125, second layer: 5223.46337890625, third layer: 2285.400634765625, output layer: 25.28683090209961\n",
      "\n",
      "Epoch 27, Test Acc: 84.11%, eta: 04:07:42 \n",
      "Averge spike counts: input layer: 2993.3623046875, second layer: 5194.5087890625, third layer: 2252.269775390625, output layer: 24.902902603149414\n",
      "\n",
      "Epoch 28, Test Acc: 83.74%, eta: 04:04:19 \n",
      "Averge spike counts: input layer: 2969.06787109375, second layer: 5134.77783203125, third layer: 2262.224853515625, output layer: 24.261533737182617\n",
      "\n",
      "Epoch 29, Test Acc: 83.07%, eta: 04:00:58 \n",
      "Averge spike counts: input layer: 3017.03662109375, second layer: 5173.26416015625, third layer: 2268.291015625, output layer: 24.376859664916992\n",
      "\n",
      "Epoch 30, Test Acc: 84.04%, eta: 03:57:37 \n",
      "Averge spike counts: input layer: 2993.3515625, second layer: 5057.93408203125, third layer: 2232.340087890625, output layer: 24.76116180419922\n",
      "\n",
      "Epoch 31, Test Acc: 85.12%, eta: 03:54:13 \n",
      "Averge spike counts: input layer: 3002.3701171875, second layer: 5063.654296875, third layer: 2233.91162109375, output layer: 24.095983505249023\n",
      "\n",
      "Epoch 32, Test Acc: 83.44%, eta: 03:50:50 \n",
      "Averge spike counts: input layer: 3018.710693359375, second layer: 5096.416015625, third layer: 2253.381103515625, output layer: 24.938615798950195\n",
      "\n",
      "Epoch 33, Test Acc: 83.97%, eta: 03:47:26 \n",
      "Averge spike counts: input layer: 3013.541015625, second layer: 5001.283203125, third layer: 2222.0849609375, output layer: 24.26116180419922\n",
      "\n",
      "Epoch 34, Test Acc: 83.82%, eta: 03:44:19 \n",
      "Averge spike counts: input layer: 2992.966552734375, second layer: 4975.66845703125, third layer: 2223.3017578125, output layer: 24.204612731933594\n",
      "\n",
      "Epoch 35, Test Acc: 84.56%, eta: 03:40:57 \n",
      "Averge spike counts: input layer: 3002.181640625, second layer: 4914.86376953125, third layer: 2196.919677734375, output layer: 23.520462036132812\n",
      "\n",
      "Epoch 36, Test Acc: 85.01%, eta: 03:37:34 \n",
      "Averge spike counts: input layer: 3010.896240234375, second layer: 4894.1650390625, third layer: 2197.504150390625, output layer: 23.158483505249023\n",
      "\n",
      "Epoch 37, Test Acc: 84.82%, eta: 03:34:13 \n",
      "Averge spike counts: input layer: 2983.338623046875, second layer: 4846.1201171875, third layer: 2173.15771484375, output layer: 23.956846237182617\n",
      "\n",
      "Epoch 38, Test Acc: 85.12%, eta: 03:30:47 \n",
      "Averge spike counts: input layer: 3007.442138671875, second layer: 4841.47021484375, third layer: 2182.43115234375, output layer: 24.161087036132812\n",
      "\n",
      "Epoch 39, Test Acc: 84.75%, eta: 03:27:23 \n",
      "Averge spike counts: input layer: 2951.376220703125, second layer: 4672.8056640625, third layer: 2135.482666015625, output layer: 22.954612731933594\n",
      "\n",
      "Epoch 40, Test Acc: 85.12%, eta: 03:24:02 \n",
      "Averge spike counts: input layer: 2998.613525390625, second layer: 4718.1064453125, third layer: 2119.537353515625, output layer: 22.961681365966797\n",
      "\n",
      "Epoch 41, Test Acc: 85.19%, eta: 03:20:39 \n",
      "Averge spike counts: input layer: 2961.322509765625, second layer: 4668.35888671875, third layer: 2112.262939453125, output layer: 23.847471237182617\n",
      "\n",
      "Epoch 42, Test Acc: 84.19%, eta: 03:17:14 \n",
      "Averge spike counts: input layer: 2996.864990234375, second layer: 4648.8056640625, third layer: 2097.905517578125, output layer: 22.623512268066406\n",
      "\n",
      "Epoch 43, Test Acc: 84.90%, eta: 03:13:49 \n",
      "Averge spike counts: input layer: 2952.421875, second layer: 4592.56787109375, third layer: 2085.71240234375, output layer: 23.043155670166016\n",
      "\n",
      "Epoch 44, Test Acc: 84.67%, eta: 03:10:25 \n",
      "Averge spike counts: input layer: 2950.426025390625, second layer: 4532.501953125, third layer: 2070.8984375, output layer: 22.3359375\n",
      "\n",
      "Epoch 45, Test Acc: 84.26%, eta: 03:07:01 \n",
      "Averge spike counts: input layer: 2940.14404296875, second layer: 4499.67626953125, third layer: 2059.246337890625, output layer: 22.498512268066406\n",
      "\n",
      "Epoch 46, Test Acc: 84.78%, eta: 03:03:37 \n",
      "Averge spike counts: input layer: 2935.894775390625, second layer: 4508.498046875, third layer: 2055.779052734375, output layer: 22.680431365966797\n",
      "\n",
      "Epoch 47, Test Acc: 86.61%, eta: 03:00:13 \n",
      "Averge spike counts: input layer: 2906.084228515625, second layer: 4449.13037109375, third layer: 2051.174560546875, output layer: 23.40364646911621\n",
      "\n",
      "Epoch 48, Test Acc: 85.27%, eta: 02:56:49 \n",
      "Averge spike counts: input layer: 2917.314453125, second layer: 4433.794921875, third layer: 2056.813232421875, output layer: 23.274181365966797\n",
      "\n",
      "Epoch 49, Test Acc: 84.82%, eta: 02:53:24 \n",
      "Averge spike counts: input layer: 2917.104248046875, second layer: 4397.8935546875, third layer: 2052.045166015625, output layer: 22.645090103149414\n",
      "\n",
      "Epoch 50, Test Acc: 85.01%, eta: 02:50:00 \n",
      "Averge spike counts: input layer: 2954.96630859375, second layer: 4468.41357421875, third layer: 2064.160400390625, output layer: 23.375\n",
      "\n",
      "Epoch 51, Test Acc: 84.52%, eta: 02:46:36 \n",
      "Averge spike counts: input layer: 2901.07568359375, second layer: 4327.78125, third layer: 2014.0904541015625, output layer: 22.553571701049805\n",
      "\n",
      "Epoch 52, Test Acc: 85.53%, eta: 02:43:12 \n",
      "Averge spike counts: input layer: 2928.7099609375, second layer: 4352.47119140625, third layer: 2041.7734375, output layer: 22.932292938232422\n",
      "\n",
      "Epoch 53, Test Acc: 85.60%, eta: 02:39:47 \n",
      "Averge spike counts: input layer: 2907.19677734375, second layer: 4316.71630859375, third layer: 2033.27685546875, output layer: 22.056175231933594\n",
      "\n",
      "Epoch 54, Test Acc: 85.53%, eta: 02:36:22 \n",
      "Averge spike counts: input layer: 2887.22705078125, second layer: 4283.67626953125, third layer: 2029.3092041015625, output layer: 21.928199768066406\n",
      "\n",
      "Epoch 55, Test Acc: 85.97%, eta: 02:32:59 \n",
      "Averge spike counts: input layer: 2839.0732421875, second layer: 4212.16455078125, third layer: 1996.4959716796875, output layer: 21.58110237121582\n",
      "\n",
      "Epoch 56, Test Acc: 83.67%, eta: 02:29:34 \n",
      "Averge spike counts: input layer: 2887.657470703125, second layer: 4298.71337890625, third layer: 2021.6845703125, output layer: 22.490327835083008\n",
      "\n",
      "Epoch 57, Test Acc: 85.75%, eta: 02:26:10 \n",
      "Averge spike counts: input layer: 2857.1826171875, second layer: 4245.5, third layer: 1995.6885986328125, output layer: 22.44866180419922\n",
      "\n",
      "Epoch 58, Test Acc: 84.60%, eta: 02:22:46 \n",
      "Averge spike counts: input layer: 2868.210693359375, second layer: 4246.09912109375, third layer: 2009.007080078125, output layer: 22.3831844329834\n",
      "\n",
      "Epoch 59, Test Acc: 85.27%, eta: 02:19:22 \n",
      "Averge spike counts: input layer: 2880.0419921875, second layer: 4200.83984375, third layer: 1989.087890625, output layer: 22.168899536132812\n",
      "\n",
      "Epoch 60, Test Acc: 85.42%, eta: 02:15:58 \n",
      "Averge spike counts: input layer: 2885.529052734375, second layer: 4182.0087890625, third layer: 1980.2991943359375, output layer: 22.543527603149414\n",
      "\n",
      "Epoch 61, Test Acc: 85.23%, eta: 02:12:34 \n",
      "Averge spike counts: input layer: 2863.54541015625, second layer: 4110.25634765625, third layer: 1947.1495361328125, output layer: 21.514509201049805\n",
      "\n",
      "Epoch 62, Test Acc: 86.20%, eta: 02:09:09 \n",
      "Averge spike counts: input layer: 2855.201416015625, second layer: 4120.92529296875, third layer: 1958.8114013671875, output layer: 21.86160659790039\n",
      "\n",
      "Epoch 63, Test Acc: 85.42%, eta: 02:05:45 \n",
      "Averge spike counts: input layer: 2813.041748046875, second layer: 4061.626953125, third layer: 1940.71728515625, output layer: 21.603422164916992\n",
      "\n",
      "Epoch 64, Test Acc: 85.90%, eta: 02:02:21 \n",
      "Averge spike counts: input layer: 2806.28662109375, second layer: 4012.38671875, third layer: 1928.40625, output layer: 21.293899536132812\n",
      "\n",
      "Epoch 65, Test Acc: 85.01%, eta: 01:58:57 \n",
      "Averge spike counts: input layer: 2844.1279296875, second layer: 4075.337890625, third layer: 1928.3773193359375, output layer: 21.582962036132812\n",
      "\n",
      "Epoch 66, Test Acc: 86.16%, eta: 01:55:33 \n",
      "Averge spike counts: input layer: 2816.9169921875, second layer: 4003.813232421875, third layer: 1935.702392578125, output layer: 21.543899536132812\n",
      "\n",
      "Epoch 67, Test Acc: 84.78%, eta: 01:52:09 \n",
      "Averge spike counts: input layer: 2757.406005859375, second layer: 3910.742919921875, third layer: 1900.296875, output layer: 21.08928680419922\n",
      "\n",
      "Epoch 68, Test Acc: 84.67%, eta: 01:48:45 \n",
      "Averge spike counts: input layer: 2812.71435546875, second layer: 3967.62255859375, third layer: 1906.7601318359375, output layer: 21.3284969329834\n",
      "\n",
      "Epoch 69, Test Acc: 86.16%, eta: 01:45:21 \n",
      "Averge spike counts: input layer: 2769.9765625, second layer: 3885.591552734375, third layer: 1892.272705078125, output layer: 21.397321701049805\n",
      "\n",
      "Epoch 70, Test Acc: 84.67%, eta: 01:41:56 \n",
      "Averge spike counts: input layer: 2791.23974609375, second layer: 3934.49609375, third layer: 1907.8489990234375, output layer: 21.770090103149414\n",
      "\n",
      "Epoch 71, Test Acc: 85.83%, eta: 01:38:32 \n",
      "Averge spike counts: input layer: 2801.16162109375, second layer: 3946.45947265625, third layer: 1893.24560546875, output layer: 20.842634201049805\n",
      "\n",
      "Epoch 72, Test Acc: 86.38%, eta: 01:35:08 \n",
      "Averge spike counts: input layer: 2763.156982421875, second layer: 3846.0263671875, third layer: 1863.9881591796875, output layer: 20.771577835083008\n",
      "\n",
      "Epoch 73, Test Acc: 85.94%, eta: 01:31:45 \n",
      "Averge spike counts: input layer: 2755.232177734375, second layer: 3854.835693359375, third layer: 1889.9453125, output layer: 21.34114646911621\n",
      "\n",
      "Epoch 74, Test Acc: 84.75%, eta: 01:28:21 \n",
      "Averge spike counts: input layer: 2732.59716796875, second layer: 3819.0703125, third layer: 1885.524169921875, output layer: 21.126859664916992\n",
      "\n",
      "Epoch 75, Test Acc: 85.49%, eta: 01:24:57 \n",
      "Averge spike counts: input layer: 2770.505615234375, second layer: 3856.9638671875, third layer: 1890.4620361328125, output layer: 21.285343170166016\n",
      "\n",
      "Epoch 76, Test Acc: 86.79%, eta: 01:21:34 \n",
      "Averge spike counts: input layer: 2762.053955078125, second layer: 3811.046630859375, third layer: 1878.03271484375, output layer: 20.896949768066406\n",
      "\n",
      "Epoch 77, Test Acc: 86.24%, eta: 01:18:10 \n",
      "Averge spike counts: input layer: 2687.283447265625, second layer: 3725.93896484375, third layer: 1842.2236328125, output layer: 20.891368865966797\n",
      "\n",
      "Epoch 78, Test Acc: 86.31%, eta: 01:14:46 \n",
      "Averge spike counts: input layer: 2726.360595703125, second layer: 3765.69677734375, third layer: 1855.5286865234375, output layer: 20.572172164916992\n",
      "\n",
      "Epoch 79, Test Acc: 84.45%, eta: 01:11:22 \n",
      "Averge spike counts: input layer: 2759.83349609375, second layer: 3829.06884765625, third layer: 1864.463623046875, output layer: 20.9375\n",
      "\n",
      "Epoch 80, Test Acc: 85.12%, eta: 01:07:58 \n",
      "Averge spike counts: input layer: 2708.745849609375, second layer: 3750.546142578125, third layer: 1842.5755615234375, output layer: 21.209449768066406\n",
      "\n",
      "Epoch 81, Test Acc: 86.38%, eta: 01:04:34 \n",
      "Averge spike counts: input layer: 2728.448974609375, second layer: 3730.147705078125, third layer: 1834.2857666015625, output layer: 19.80989646911621\n",
      "\n",
      "Epoch 82, Test Acc: 85.23%, eta: 01:01:10 \n",
      "Averge spike counts: input layer: 2676.93603515625, second layer: 3646.2490234375, third layer: 1815.4874267578125, output layer: 20.474702835083008\n",
      "\n",
      "Epoch 83, Test Acc: 85.49%, eta: 00:57:46 \n",
      "Averge spike counts: input layer: 2724.143310546875, second layer: 3658.408203125, third layer: 1819.8077392578125, output layer: 21.051340103149414\n",
      "\n",
      "Epoch 84, Test Acc: 84.90%, eta: 00:54:22 \n",
      "Averge spike counts: input layer: 2682.060302734375, second layer: 3585.260498046875, third layer: 1788.0770263671875, output layer: 20.272321701049805\n",
      "\n",
      "Epoch 85, Test Acc: 86.16%, eta: 00:50:58 \n",
      "Averge spike counts: input layer: 2708.34228515625, second layer: 3616.93798828125, third layer: 1806.1685791015625, output layer: 20.5941219329834\n",
      "\n",
      "Epoch 86, Test Acc: 86.46%, eta: 00:47:34 \n",
      "Averge spike counts: input layer: 2660.6044921875, second layer: 3569.924560546875, third layer: 1803.98291015625, output layer: 20.561384201049805\n",
      "\n",
      "Epoch 87, Test Acc: 85.57%, eta: 00:44:10 \n",
      "Averge spike counts: input layer: 2684.9765625, second layer: 3601.6689453125, third layer: 1804.8511962890625, output layer: 20.7191219329834\n",
      "\n",
      "Epoch 88, Test Acc: 86.68%, eta: 00:40:46 \n",
      "Averge spike counts: input layer: 2661.463623046875, second layer: 3572.1044921875, third layer: 1789.3148193359375, output layer: 20.08891487121582\n",
      "\n",
      "Epoch 89, Test Acc: 86.05%, eta: 00:37:22 \n",
      "Averge spike counts: input layer: 2670.16162109375, second layer: 3615.94140625, third layer: 1798.7943115234375, output layer: 20.892112731933594\n",
      "\n",
      "Epoch 90, Test Acc: 86.68%, eta: 00:33:58 \n",
      "Averge spike counts: input layer: 2661.476318359375, second layer: 3578.6640625, third layer: 1795.682373046875, output layer: 20.578868865966797\n",
      "\n",
      "Epoch 91, Test Acc: 85.68%, eta: 00:30:35 \n",
      "Averge spike counts: input layer: 2651.25146484375, second layer: 3569.617919921875, third layer: 1794.47802734375, output layer: 20.4769344329834\n",
      "\n",
      "Epoch 92, Test Acc: 85.23%, eta: 00:27:11 \n",
      "Averge spike counts: input layer: 2650.548828125, second layer: 3528.693603515625, third layer: 1789.6287841796875, output layer: 20.164806365966797\n",
      "\n",
      "Epoch 93, Test Acc: 84.75%, eta: 00:23:47 \n",
      "Averge spike counts: input layer: 2709.970703125, second layer: 3620.2392578125, third layer: 1789.0989990234375, output layer: 20.29464340209961\n",
      "\n",
      "Epoch 94, Test Acc: 86.38%, eta: 00:20:23 \n",
      "Averge spike counts: input layer: 2653.674072265625, second layer: 3566.48486328125, third layer: 1787.2679443359375, output layer: 20.389137268066406\n",
      "\n",
      "Epoch 95, Test Acc: 87.02%, eta: 00:16:59 \n",
      "Averge spike counts: input layer: 2664.298095703125, second layer: 3583.725830078125, third layer: 1782.1607666015625, output layer: 20.5074405670166\n",
      "\n",
      "Epoch 96, Test Acc: 87.02%, eta: 00:13:35 \n",
      "Averge spike counts: input layer: 2617.39111328125, second layer: 3508.890380859375, third layer: 1762.673828125, output layer: 20.253721237182617\n",
      "\n",
      "Epoch 97, Test Acc: 86.38%, eta: 00:10:11 \n",
      "Averge spike counts: input layer: 2646.097412109375, second layer: 3537.872802734375, third layer: 1767.9249267578125, output layer: 20.282737731933594\n",
      "\n",
      "Epoch 98, Test Acc: 86.61%, eta: 00:06:47 \n",
      "Averge spike counts: input layer: 2612.081787109375, second layer: 3454.161865234375, third layer: 1774.980712890625, output layer: 20.012277603149414\n",
      "\n",
      "Epoch 99, Test Acc: 86.24%, eta: 00:03:23 \n",
      "Averge spike counts: input layer: 2612.546875, second layer: 3481.796630859375, third layer: 1765.180419921875, output layer: 20.435640335083008\n",
      "\n",
      "Epoch 100, Test Acc: 86.42%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 2619.416748046875, second layer: 3473.0830078125, third layer: 1765.661865234375, output layer: 20.253721237182617\n",
      "\n",
      "20387.961980104446 seconds\n",
      "\n",
      "zero reset mechanism experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_zero_reset = Net_reset_zero().to(device)\n",
    "net_zero_reset = train(net_zero_reset)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"zero reset mechanism experiment\\n\")\n",
    "torch.save(net_zero_reset.state_dict(), 'model_zero_reset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Acc: 51.60%, eta: 05:37:14 \n",
      "Averge spike counts: input layer: 1891.87060546875, second layer: 2789.972412109375, third layer: 1047.5416259765625, output layer: 11.316964149475098\n",
      "\n",
      "Epoch 2, Test Acc: 69.98%, eta: 05:33:24 \n",
      "Averge spike counts: input layer: 2363.39501953125, second layer: 4565.57958984375, third layer: 2034.0294189453125, output layer: 26.93266487121582\n",
      "\n",
      "Epoch 3, Test Acc: 75.04%, eta: 05:29:59 \n",
      "Averge spike counts: input layer: 2560.67626953125, second layer: 5335.7890625, third layer: 2429.09375, output layer: 28.73772430419922\n",
      "\n",
      "Epoch 4, Test Acc: 75.63%, eta: 05:26:29 \n",
      "Averge spike counts: input layer: 2680.72802734375, second layer: 5715.58154296875, third layer: 2614.805419921875, output layer: 29.21651840209961\n",
      "\n",
      "Epoch 5, Test Acc: 78.42%, eta: 05:23:10 \n",
      "Averge spike counts: input layer: 2708.320068359375, second layer: 5829.62255859375, third layer: 2692.345703125, output layer: 29.061756134033203\n",
      "\n",
      "Epoch 6, Test Acc: 78.27%, eta: 05:19:40 \n",
      "Averge spike counts: input layer: 2796.923095703125, second layer: 6028.24951171875, third layer: 2796.222412109375, output layer: 29.44866180419922\n",
      "\n",
      "Epoch 7, Test Acc: 79.84%, eta: 05:16:12 \n",
      "Averge spike counts: input layer: 2796.016845703125, second layer: 5962.59423828125, third layer: 2768.469482421875, output layer: 28.84151840209961\n",
      "\n",
      "Epoch 8, Test Acc: 79.72%, eta: 05:13:01 \n",
      "Averge spike counts: input layer: 2774.32958984375, second layer: 5858.298828125, third layer: 2716.651123046875, output layer: 28.238468170166016\n",
      "\n",
      "Epoch 9, Test Acc: 81.06%, eta: 05:09:40 \n",
      "Averge spike counts: input layer: 2812.212890625, second layer: 5917.41845703125, third layer: 2779.4755859375, output layer: 27.8050594329834\n",
      "\n",
      "Epoch 10, Test Acc: 81.03%, eta: 05:06:13 \n",
      "Averge spike counts: input layer: 2812.976318359375, second layer: 5859.08251953125, third layer: 2736.74755859375, output layer: 28.353424072265625\n",
      "\n",
      "Epoch 11, Test Acc: 81.47%, eta: 05:02:45 \n",
      "Averge spike counts: input layer: 2825.63427734375, second layer: 5843.1943359375, third layer: 2737.0537109375, output layer: 28.873884201049805\n",
      "\n",
      "Epoch 12, Test Acc: 81.51%, eta: 04:59:18 \n",
      "Averge spike counts: input layer: 2892.33447265625, second layer: 5917.0849609375, third layer: 2757.144287109375, output layer: 27.777530670166016\n",
      "\n",
      "Epoch 13, Test Acc: 81.55%, eta: 04:55:51 \n",
      "Averge spike counts: input layer: 2840.63427734375, second layer: 5773.39501953125, third layer: 2717.9365234375, output layer: 27.694568634033203\n",
      "\n",
      "Epoch 14, Test Acc: 82.51%, eta: 04:52:28 \n",
      "Averge spike counts: input layer: 2844.2763671875, second layer: 5690.84130859375, third layer: 2685.288330078125, output layer: 27.90959930419922\n",
      "\n",
      "Epoch 15, Test Acc: 82.63%, eta: 04:49:04 \n",
      "Averge spike counts: input layer: 2814.49072265625, second layer: 5572.0107421875, third layer: 2648.158447265625, output layer: 27.446056365966797\n",
      "\n",
      "Epoch 16, Test Acc: 82.11%, eta: 04:45:39 \n",
      "Averge spike counts: input layer: 2893.129150390625, second layer: 5667.26123046875, third layer: 2664.075927734375, output layer: 27.902530670166016\n",
      "\n",
      "Epoch 17, Test Acc: 81.58%, eta: 04:42:24 \n",
      "Averge spike counts: input layer: 2887.91748046875, second layer: 5620.0791015625, third layer: 2659.81103515625, output layer: 28.505952835083008\n",
      "\n",
      "Epoch 18, Test Acc: 82.81%, eta: 04:38:58 \n",
      "Averge spike counts: input layer: 2845.758544921875, second layer: 5472.08544921875, third layer: 2611.960205078125, output layer: 27.8902530670166\n",
      "\n",
      "Epoch 19, Test Acc: 81.32%, eta: 04:35:36 \n",
      "Averge spike counts: input layer: 2844.722900390625, second layer: 5422.9482421875, third layer: 2606.834228515625, output layer: 26.733259201049805\n",
      "\n",
      "Epoch 20, Test Acc: 83.56%, eta: 04:32:11 \n",
      "Averge spike counts: input layer: 2836.18115234375, second layer: 5346.62451171875, third layer: 2565.060791015625, output layer: 26.806547164916992\n",
      "\n",
      "Epoch 21, Test Acc: 82.44%, eta: 04:28:47 \n",
      "Averge spike counts: input layer: 2843.622802734375, second layer: 5293.248046875, third layer: 2546.2998046875, output layer: 26.23772430419922\n",
      "\n",
      "Epoch 22, Test Acc: 82.89%, eta: 04:25:22 \n",
      "Averge spike counts: input layer: 2842.452392578125, second layer: 5233.60009765625, third layer: 2527.673095703125, output layer: 26.267858505249023\n",
      "\n",
      "Epoch 23, Test Acc: 82.55%, eta: 04:21:56 \n",
      "Averge spike counts: input layer: 2870.345947265625, second layer: 5277.6474609375, third layer: 2560.062255859375, output layer: 27.033483505249023\n",
      "\n",
      "Epoch 24, Test Acc: 83.26%, eta: 04:18:31 \n",
      "Averge spike counts: input layer: 2876.65185546875, second layer: 5191.72412109375, third layer: 2513.972900390625, output layer: 26.202009201049805\n",
      "\n",
      "Epoch 25, Test Acc: 84.56%, eta: 04:15:07 \n",
      "Averge spike counts: input layer: 2818.668212890625, second layer: 5072.349609375, third layer: 2481.41552734375, output layer: 27.27901840209961\n",
      "\n",
      "Epoch 26, Test Acc: 83.93%, eta: 04:11:48 \n",
      "Averge spike counts: input layer: 2886.90185546875, second layer: 5143.04638671875, third layer: 2508.340087890625, output layer: 27.115699768066406\n",
      "\n",
      "Epoch 27, Test Acc: 81.21%, eta: 04:08:31 \n",
      "Averge spike counts: input layer: 2832.178955078125, second layer: 4985.2373046875, third layer: 2452.58740234375, output layer: 26.443824768066406\n",
      "\n",
      "Epoch 28, Test Acc: 83.37%, eta: 04:05:09 \n",
      "Averge spike counts: input layer: 2808.5751953125, second layer: 4903.4404296875, third layer: 2418.203125, output layer: 24.283855438232422\n",
      "\n",
      "Epoch 29, Test Acc: 83.48%, eta: 04:01:44 \n",
      "Averge spike counts: input layer: 2854.2958984375, second layer: 4962.0107421875, third layer: 2458.59912109375, output layer: 25.953125\n",
      "\n",
      "Epoch 30, Test Acc: 85.34%, eta: 03:58:18 \n",
      "Averge spike counts: input layer: 2876.0732421875, second layer: 4948.76123046875, third layer: 2451.306640625, output layer: 26.100446701049805\n",
      "\n",
      "Epoch 31, Test Acc: 84.71%, eta: 03:54:55 \n",
      "Averge spike counts: input layer: 2791.951416015625, second layer: 4745.85205078125, third layer: 2382.56689453125, output layer: 26.031993865966797\n",
      "\n",
      "Epoch 32, Test Acc: 82.81%, eta: 03:51:30 \n",
      "Averge spike counts: input layer: 2847.2021484375, second layer: 4799.60595703125, third layer: 2383.462158203125, output layer: 24.954984664916992\n",
      "\n",
      "Epoch 33, Test Acc: 85.79%, eta: 03:48:04 \n",
      "Averge spike counts: input layer: 2837.5107421875, second layer: 4802.14404296875, third layer: 2392.27978515625, output layer: 25.6331844329834\n",
      "\n",
      "Epoch 34, Test Acc: 85.19%, eta: 03:44:40 \n",
      "Averge spike counts: input layer: 2759.923095703125, second layer: 4601.64306640625, third layer: 2317.84228515625, output layer: 24.498140335083008\n",
      "\n",
      "Epoch 35, Test Acc: 84.56%, eta: 03:41:17 \n",
      "Averge spike counts: input layer: 2822.321044921875, second layer: 4648.7197265625, third layer: 2331.514892578125, output layer: 24.548734664916992\n",
      "\n",
      "Epoch 36, Test Acc: 83.85%, eta: 03:37:51 \n",
      "Averge spike counts: input layer: 2846.2734375, second layer: 4679.5703125, third layer: 2348.616455078125, output layer: 24.786087036132812\n",
      "\n",
      "Epoch 37, Test Acc: 84.19%, eta: 03:34:27 \n",
      "Averge spike counts: input layer: 2835.619384765625, second layer: 4579.8681640625, third layer: 2312.225830078125, output layer: 23.826265335083008\n",
      "\n",
      "Epoch 38, Test Acc: 84.71%, eta: 03:31:02 \n",
      "Averge spike counts: input layer: 2810.172607421875, second layer: 4563.91357421875, third layer: 2308.822509765625, output layer: 24.673362731933594\n",
      "\n",
      "Epoch 39, Test Acc: 84.23%, eta: 03:27:37 \n",
      "Averge spike counts: input layer: 2842.538818359375, second layer: 4609.1591796875, third layer: 2332.1240234375, output layer: 24.293527603149414\n",
      "\n",
      "Epoch 40, Test Acc: 84.04%, eta: 03:24:12 \n",
      "Averge spike counts: input layer: 2879.377197265625, second layer: 4697.00390625, third layer: 2380.063720703125, output layer: 26.061384201049805\n",
      "\n",
      "Epoch 41, Test Acc: 84.90%, eta: 03:20:49 \n",
      "Averge spike counts: input layer: 2830.90625, second layer: 4545.185546875, third layer: 2320.0146484375, output layer: 25.47395896911621\n",
      "\n",
      "Epoch 42, Test Acc: 84.04%, eta: 03:17:24 \n",
      "Averge spike counts: input layer: 2732.11572265625, second layer: 4344.7265625, third layer: 2262.283203125, output layer: 25.274925231933594\n",
      "\n",
      "Epoch 43, Test Acc: 83.44%, eta: 03:13:59 \n",
      "Averge spike counts: input layer: 2776.92724609375, second layer: 4392.638671875, third layer: 2258.158203125, output layer: 25.102306365966797\n",
      "\n",
      "Epoch 44, Test Acc: 85.42%, eta: 03:10:36 \n",
      "Averge spike counts: input layer: 2781.315185546875, second layer: 4374.57373046875, third layer: 2245.77099609375, output layer: 25.117931365966797\n",
      "\n",
      "Epoch 45, Test Acc: 84.56%, eta: 03:07:11 \n",
      "Averge spike counts: input layer: 2802.347900390625, second layer: 4366.6015625, third layer: 2240.830078125, output layer: 24.2816219329834\n",
      "\n",
      "Epoch 46, Test Acc: 85.83%, eta: 03:03:48 \n",
      "Averge spike counts: input layer: 2777.629150390625, second layer: 4299.66845703125, third layer: 2215.735595703125, output layer: 23.939733505249023\n",
      "\n",
      "Epoch 47, Test Acc: 84.41%, eta: 03:00:23 \n",
      "Averge spike counts: input layer: 2753.5048828125, second layer: 4271.1650390625, third layer: 2220.27978515625, output layer: 23.933408737182617\n",
      "\n",
      "Epoch 48, Test Acc: 85.75%, eta: 02:56:59 \n",
      "Averge spike counts: input layer: 2723.6220703125, second layer: 4238.13720703125, third layer: 2208.751953125, output layer: 24.2418155670166\n",
      "\n",
      "Epoch 49, Test Acc: 84.00%, eta: 02:53:34 \n",
      "Averge spike counts: input layer: 2775.918212890625, second layer: 4240.45849609375, third layer: 2194.96875, output layer: 24.671875\n",
      "\n",
      "Epoch 50, Test Acc: 85.45%, eta: 02:50:10 \n",
      "Averge spike counts: input layer: 2760.482177734375, second layer: 4200.048828125, third layer: 2172.649169921875, output layer: 23.985490798950195\n",
      "\n",
      "Epoch 51, Test Acc: 85.83%, eta: 02:46:45 \n",
      "Averge spike counts: input layer: 2774.43408203125, second layer: 4228.1455078125, third layer: 2189.502197265625, output layer: 24.616443634033203\n",
      "\n",
      "Epoch 52, Test Acc: 85.68%, eta: 02:43:22 \n",
      "Averge spike counts: input layer: 2769.93798828125, second layer: 4209.453125, third layer: 2192.746337890625, output layer: 24.133928298950195\n",
      "\n",
      "Epoch 53, Test Acc: 84.38%, eta: 02:39:57 \n",
      "Averge spike counts: input layer: 2735.0576171875, second layer: 4116.00830078125, third layer: 2170.200439453125, output layer: 23.728422164916992\n",
      "\n",
      "Epoch 54, Test Acc: 83.97%, eta: 02:36:33 \n",
      "Averge spike counts: input layer: 2758.6083984375, second layer: 4176.28759765625, third layer: 2184.72021484375, output layer: 23.846355438232422\n",
      "\n",
      "Epoch 55, Test Acc: 85.71%, eta: 02:33:09 \n",
      "Averge spike counts: input layer: 2747.44384765625, second layer: 4095.879150390625, third layer: 2166.257080078125, output layer: 23.717634201049805\n",
      "\n",
      "Epoch 56, Test Acc: 85.12%, eta: 02:29:45 \n",
      "Averge spike counts: input layer: 2750.958740234375, second layer: 4039.6708984375, third layer: 2140.735595703125, output layer: 23.534971237182617\n",
      "\n",
      "Epoch 57, Test Acc: 84.41%, eta: 02:26:21 \n",
      "Averge spike counts: input layer: 2705.670166015625, second layer: 4015.390625, third layer: 2146.90673828125, output layer: 24.277158737182617\n",
      "\n",
      "Epoch 58, Test Acc: 84.52%, eta: 02:22:56 \n",
      "Averge spike counts: input layer: 2733.157470703125, second layer: 4051.90478515625, third layer: 2148.114501953125, output layer: 24.188243865966797\n",
      "\n",
      "Epoch 59, Test Acc: 85.38%, eta: 02:19:32 \n",
      "Averge spike counts: input layer: 2712.66552734375, second layer: 3910.8125, third layer: 2123.132568359375, output layer: 22.588171005249023\n",
      "\n",
      "Epoch 60, Test Acc: 86.05%, eta: 02:16:08 \n",
      "Averge spike counts: input layer: 2693.1005859375, second layer: 3889.9326171875, third layer: 2102.661376953125, output layer: 22.808780670166016\n",
      "\n",
      "Epoch 61, Test Acc: 85.75%, eta: 02:12:45 \n",
      "Averge spike counts: input layer: 2693.011962890625, second layer: 3882.060791015625, third layer: 2116.4169921875, output layer: 23.2183780670166\n",
      "\n",
      "Epoch 62, Test Acc: 85.86%, eta: 02:09:20 \n",
      "Averge spike counts: input layer: 2657.881103515625, second layer: 3850.160400390625, third layer: 2096.013916015625, output layer: 23.048734664916992\n",
      "\n",
      "Epoch 63, Test Acc: 84.93%, eta: 02:05:57 \n",
      "Averge spike counts: input layer: 2677.23193359375, second layer: 3853.50830078125, third layer: 2090.827392578125, output layer: 22.853796005249023\n",
      "\n",
      "Epoch 64, Test Acc: 85.27%, eta: 02:02:33 \n",
      "Averge spike counts: input layer: 2700.57373046875, second layer: 3892.617919921875, third layer: 2103.81298828125, output layer: 23.548362731933594\n",
      "\n",
      "Epoch 65, Test Acc: 85.31%, eta: 01:59:08 \n",
      "Averge spike counts: input layer: 2696.03173828125, second layer: 3895.38037109375, third layer: 2099.931884765625, output layer: 23.219865798950195\n",
      "\n",
      "Epoch 66, Test Acc: 84.60%, eta: 01:55:45 \n",
      "Averge spike counts: input layer: 2661.8818359375, second layer: 3800.235595703125, third layer: 2057.180419921875, output layer: 22.40178680419922\n",
      "\n",
      "Epoch 67, Test Acc: 85.64%, eta: 01:52:20 \n",
      "Averge spike counts: input layer: 2650.0703125, second layer: 3747.88037109375, third layer: 2035.921142578125, output layer: 22.168899536132812\n",
      "\n",
      "Epoch 68, Test Acc: 83.59%, eta: 01:48:56 \n",
      "Averge spike counts: input layer: 2673.37158203125, second layer: 3785.259033203125, third layer: 2045.0186767578125, output layer: 23.124256134033203\n",
      "\n",
      "Epoch 69, Test Acc: 84.60%, eta: 01:45:31 \n",
      "Averge spike counts: input layer: 2660.03173828125, second layer: 3783.127197265625, third layer: 2049.388427734375, output layer: 22.941221237182617\n",
      "\n",
      "Epoch 70, Test Acc: 85.86%, eta: 01:42:07 \n",
      "Averge spike counts: input layer: 2647.84130859375, second layer: 3707.6201171875, third layer: 2020.47509765625, output layer: 22.882068634033203\n",
      "\n",
      "Epoch 71, Test Acc: 85.97%, eta: 01:38:43 \n",
      "Averge spike counts: input layer: 2613.251220703125, second layer: 3680.937255859375, third layer: 2017.8128662109375, output layer: 22.9527530670166\n",
      "\n",
      "Epoch 72, Test Acc: 84.86%, eta: 01:35:19 \n",
      "Averge spike counts: input layer: 2625.167724609375, second layer: 3665.48828125, third layer: 2017.783935546875, output layer: 22.457962036132812\n",
      "\n",
      "Epoch 73, Test Acc: 85.49%, eta: 01:31:55 \n",
      "Averge spike counts: input layer: 2617.12255859375, second layer: 3659.723388671875, third layer: 2005.10498046875, output layer: 22.4840030670166\n",
      "\n",
      "Epoch 74, Test Acc: 85.79%, eta: 01:28:31 \n",
      "Averge spike counts: input layer: 2634.34033203125, second layer: 3674.119140625, third layer: 2017.77197265625, output layer: 22.297618865966797\n",
      "\n",
      "Epoch 75, Test Acc: 85.75%, eta: 01:25:07 \n",
      "Averge spike counts: input layer: 2606.989990234375, second layer: 3606.643310546875, third layer: 1991.110107421875, output layer: 22.209077835083008\n",
      "\n",
      "Epoch 76, Test Acc: 85.57%, eta: 01:21:42 \n",
      "Averge spike counts: input layer: 2628.201416015625, second layer: 3636.636962890625, third layer: 2009.19384765625, output layer: 22.705358505249023\n",
      "\n",
      "Epoch 77, Test Acc: 85.23%, eta: 01:18:18 \n",
      "Averge spike counts: input layer: 2617.705810546875, second layer: 3597.988525390625, third layer: 2013.2247314453125, output layer: 22.96614646911621\n",
      "\n",
      "Epoch 78, Test Acc: 85.19%, eta: 01:14:53 \n",
      "Averge spike counts: input layer: 2586.0693359375, second layer: 3597.660400390625, third layer: 1999.852294921875, output layer: 23.1636905670166\n",
      "\n",
      "Epoch 79, Test Acc: 86.05%, eta: 01:11:33 \n",
      "Averge spike counts: input layer: 2588.03564453125, second layer: 3547.301025390625, third layer: 1977.254150390625, output layer: 22.550968170166016\n",
      "\n",
      "Epoch 80, Test Acc: 85.57%, eta: 01:08:08 \n",
      "Averge spike counts: input layer: 2573.818603515625, second layer: 3495.337158203125, third layer: 1960.2091064453125, output layer: 21.73214340209961\n",
      "\n",
      "Epoch 81, Test Acc: 86.79%, eta: 01:04:44 \n",
      "Averge spike counts: input layer: 2593.8095703125, second layer: 3527.369873046875, third layer: 1966.60009765625, output layer: 22.48958396911621\n",
      "\n",
      "Epoch 82, Test Acc: 85.75%, eta: 01:01:19 \n",
      "Averge spike counts: input layer: 2572.15283203125, second layer: 3484.791259765625, third layer: 1960.5948486328125, output layer: 22.39583396911621\n",
      "\n",
      "Epoch 83, Test Acc: 84.93%, eta: 00:57:55 \n",
      "Averge spike counts: input layer: 2579.70068359375, second layer: 3489.29248046875, third layer: 1971.3526611328125, output layer: 22.2105655670166\n",
      "\n",
      "Epoch 84, Test Acc: 85.90%, eta: 00:54:31 \n",
      "Averge spike counts: input layer: 2570.59130859375, second layer: 3446.148193359375, third layer: 1958.7347412109375, output layer: 21.406993865966797\n",
      "\n",
      "Epoch 85, Test Acc: 86.09%, eta: 00:51:06 \n",
      "Averge spike counts: input layer: 2591.4091796875, second layer: 3511.7939453125, third layer: 1963.47216796875, output layer: 21.961681365966797\n",
      "\n",
      "Epoch 86, Test Acc: 85.12%, eta: 00:47:42 \n",
      "Averge spike counts: input layer: 2564.3310546875, second layer: 3477.871337890625, third layer: 1947.5045166015625, output layer: 21.752605438232422\n",
      "\n",
      "Epoch 87, Test Acc: 84.19%, eta: 00:44:17 \n",
      "Averge spike counts: input layer: 2571.962890625, second layer: 3510.53271484375, third layer: 1949.355712890625, output layer: 22.1558780670166\n",
      "\n",
      "Epoch 88, Test Acc: 84.52%, eta: 00:40:53 \n",
      "Averge spike counts: input layer: 2564.026123046875, second layer: 3478.05224609375, third layer: 1957.4456787109375, output layer: 22.221355438232422\n",
      "\n",
      "Epoch 89, Test Acc: 85.83%, eta: 00:37:29 \n",
      "Averge spike counts: input layer: 2544.97314453125, second layer: 3381.27978515625, third layer: 1915.863525390625, output layer: 21.072172164916992\n",
      "\n",
      "Epoch 90, Test Acc: 85.57%, eta: 00:34:04 \n",
      "Averge spike counts: input layer: 2574.970947265625, second layer: 3479.439453125, third layer: 1956.7850341796875, output layer: 22.09933090209961\n",
      "\n",
      "Epoch 91, Test Acc: 86.20%, eta: 00:30:40 \n",
      "Averge spike counts: input layer: 2513.38623046875, second layer: 3405.799072265625, third layer: 1929.60791015625, output layer: 21.7886905670166\n",
      "\n",
      "Epoch 92, Test Acc: 85.97%, eta: 00:27:15 \n",
      "Averge spike counts: input layer: 2535.06298828125, second layer: 3439.519775390625, third layer: 1932.525390625, output layer: 22.470983505249023\n",
      "\n",
      "Epoch 93, Test Acc: 86.31%, eta: 00:23:51 \n",
      "Averge spike counts: input layer: 2509.75439453125, second layer: 3406.186767578125, third layer: 1925.7303466796875, output layer: 22.275671005249023\n",
      "\n",
      "Epoch 94, Test Acc: 85.94%, eta: 00:20:26 \n",
      "Averge spike counts: input layer: 2536.168212890625, second layer: 3391.84130859375, third layer: 1921.8671875, output layer: 21.774553298950195\n",
      "\n",
      "Epoch 95, Test Acc: 86.38%, eta: 00:17:02 \n",
      "Averge spike counts: input layer: 2523.164794921875, second layer: 3363.8818359375, third layer: 1916.55322265625, output layer: 21.90178680419922\n",
      "\n",
      "Epoch 96, Test Acc: 86.83%, eta: 00:13:37 \n",
      "Averge spike counts: input layer: 2504.21240234375, second layer: 3353.421875, third layer: 1912.2999267578125, output layer: 22.175596237182617\n",
      "\n",
      "Epoch 97, Test Acc: 86.09%, eta: 00:10:13 \n",
      "Averge spike counts: input layer: 2509.01318359375, second layer: 3336.993408203125, third layer: 1900.68310546875, output layer: 22.03645896911621\n",
      "\n",
      "Epoch 98, Test Acc: 86.50%, eta: 00:06:48 \n",
      "Averge spike counts: input layer: 2499.016357421875, second layer: 3290.6513671875, third layer: 1893.150390625, output layer: 21.3359375\n",
      "\n",
      "Epoch 99, Test Acc: 86.50%, eta: 00:03:24 \n",
      "Averge spike counts: input layer: 2499.83447265625, second layer: 3284.010009765625, third layer: 1904.185302734375, output layer: 21.26302146911621\n",
      "\n",
      "Epoch 100, Test Acc: 86.42%, eta: 00:00:00 \n",
      "Averge spike counts: input layer: 2501.0732421875, second layer: 3305.259765625, third layer: 1887.392578125, output layer: 21.637277603149414\n",
      "\n",
      "20446.808925628662 seconds\n",
      "\n",
      "subtract reset mechanism experiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "net_subtract_reset = Net_reset_subtract().to(device)\n",
    "net_subtract_reset = train(net_subtract_reset)\n",
    "print(time.time()-time_start, \"seconds\\n\")\n",
    "print(\"subtract reset mechanism experiment\\n\")\n",
    "torch.save(net_subtract_reset.state_dict(), 'model_subtract_reset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPJygzupZ9Y2Ke+ztot7eH8",
   "gpuType": "V100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
